[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de tweets",
    "section": "",
    "text": "Lista de librerías requeridas para el proyecto:\npip install minet\npip install pandas\npip install numpy\npip install plotly\npip install emoji\npip install spacy\npip install tweet-preprocessor\npip install bertopic\npython -m spacy download es_core_news_sm\npython -m spacy download pt_core_news_sm\n\n\n\npandas\nnumpy\nspacy\npreprocessor\nemoji\nbertopic\nplotly\n\n\n\n\n\n# read data\nimport pandas as pd\ndata = pd.read_csv('./data/raw/users.csv')\ndata\n\n\n\n\n\n\n\n\nnombre\nusername\npais\n\n\n\n\n0\nNikolas Ferreira\nnikolas_dm\nBrasil\n\n\n1\nPastor Silas Malafaia\nPastorMalafaia\nBrasil\n\n\n2\nBrasil Sem Aborto\nbrasilsemaborto\nBrasil\n\n\n3\nMisión Paz\nmisionpaz_\nColombia\n\n\n4\nMaría Fernanda Cabal\nMariaFdaCabal\nColombia\n\n\n5\nUnidos por la Vida\nUnidosxlaVidaCo\nColombia\n\n\n6\nMamela Fiallo Flor\nMamelaFialloFlo\nEcuador\n\n\n7\nEsteban Torres Cobo\netorrescobo\nEcuador\n\n\n8\nFamilia Ecuador\n_FamiliaEcuador\nEcuador\n\n\n9\nRoy Santos\nRoySantosC\nHonduras\n\n\n10\nTomás Zambrano\nTommyZambranoM\nHonduras\n\n\n11\nLuis Felipe Faraj\nluisffaraj\nHonduras\n\n\n\n\n\n\n\n\n\n\nRecolectamos los tweets de cada usuario usando la librería minet como interfaz de línea de comandos (CLI)\nminet twitter scrape tweets 'from:username' &gt; username.csv"
  },
  {
    "objectID": "index.html#instalación",
    "href": "index.html#instalación",
    "title": "Análisis de tweets",
    "section": "",
    "text": "Lista de librerías requeridas para el proyecto:\npip install minet\npip install pandas\npip install numpy\npip install plotly\npip install emoji\npip install spacy\npip install tweet-preprocessor\npip install bertopic\npython -m spacy download es_core_news_sm\npython -m spacy download pt_core_news_sm\n\n\n\npandas\nnumpy\nspacy\npreprocessor\nemoji\nbertopic\nplotly\n\n\n\n\n\n# read data\nimport pandas as pd\ndata = pd.read_csv('./data/raw/users.csv')\ndata\n\n\n\n\n\n\n\n\nnombre\nusername\npais\n\n\n\n\n0\nNikolas Ferreira\nnikolas_dm\nBrasil\n\n\n1\nPastor Silas Malafaia\nPastorMalafaia\nBrasil\n\n\n2\nBrasil Sem Aborto\nbrasilsemaborto\nBrasil\n\n\n3\nMisión Paz\nmisionpaz_\nColombia\n\n\n4\nMaría Fernanda Cabal\nMariaFdaCabal\nColombia\n\n\n5\nUnidos por la Vida\nUnidosxlaVidaCo\nColombia\n\n\n6\nMamela Fiallo Flor\nMamelaFialloFlo\nEcuador\n\n\n7\nEsteban Torres Cobo\netorrescobo\nEcuador\n\n\n8\nFamilia Ecuador\n_FamiliaEcuador\nEcuador\n\n\n9\nRoy Santos\nRoySantosC\nHonduras\n\n\n10\nTomás Zambrano\nTommyZambranoM\nHonduras\n\n\n11\nLuis Felipe Faraj\nluisffaraj\nHonduras\n\n\n\n\n\n\n\n\n\n\nRecolectamos los tweets de cada usuario usando la librería minet como interfaz de línea de comandos (CLI)\nminet twitter scrape tweets 'from:username' &gt; username.csv"
  },
  {
    "objectID": "pages/nikolas_dm.html",
    "href": "pages/nikolas_dm.html",
    "title": "Análisis de tweets de @nikolas_dm",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2012-08-15 hasta 2023-03-21\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 7035 entries, 3582 to 10616\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                            \n---  ------                   --------------  -----                            \n 0   query                    7035 non-null   object                           \n 1   id                       7035 non-null   float64                          \n 2   timestamp_utc            7035 non-null   int64                            \n 3   local_time               7035 non-null   object                           \n 4   user_screen_name         7035 non-null   object                           \n 5   text                     7035 non-null   object                           \n 6   possibly_sensitive       1841 non-null   object                           \n 7   retweet_count            7035 non-null   float64                          \n 8   like_count               7035 non-null   float64                          \n 9   reply_count              7035 non-null   float64                          \n 10  impression_count         266 non-null    object                           \n 11  lang                     7035 non-null   object                           \n 12  to_username              2197 non-null   object                           \n 13  to_userid                2197 non-null   float64                          \n 14  to_tweetid               2168 non-null   float64                          \n 15  source_name              7035 non-null   object                           \n 16  source_url               7035 non-null   object                           \n 17  user_location            7035 non-null   object                           \n 18  lat                      0 non-null      object                           \n 19  lng                      0 non-null      object                           \n 20  user_id                  7035 non-null   object                           \n 21  user_name                7035 non-null   object                           \n 22  user_verified            7035 non-null   float64                          \n 23  user_description         7035 non-null   object                           \n 24  user_url                 7035 non-null   object                           \n 25  user_image               7035 non-null   object                           \n 26  user_tweets              7035 non-null   object                           \n 27  user_followers           7035 non-null   float64                          \n 28  user_friends             7035 non-null   object                           \n 29  user_likes               7035 non-null   float64                          \n 30  user_lists               7035 non-null   float64                          \n 31  user_created_at          7035 non-null   object                           \n 32  user_timestamp_utc       7035 non-null   float64                          \n 33  collected_via            7035 non-null   object                           \n 34  match_query              7035 non-null   float64                          \n 35  retweeted_id             0 non-null      float64                          \n 36  retweeted_user           0 non-null      float64                          \n 37  retweeted_user_id        0 non-null      float64                          \n 38  retweeted_timestamp_utc  0 non-null      object                           \n 39  quoted_id                131 non-null    object                           \n 40  quoted_user              131 non-null    object                           \n 41  quoted_user_id           131 non-null    float64                          \n 42  quoted_timestamp_utc     131 non-null    float64                          \n 43  collection_time          7035 non-null   object                           \n 44  url                      7035 non-null   object                           \n 45  place_country_code       25 non-null     object                           \n 46  place_name               25 non-null     object                           \n 47  place_type               25 non-null     object                           \n 48  place_coordinates        25 non-null     object                           \n 49  links                    372 non-null    object                           \n 50  domains                  372 non-null    object                           \n 51  media_urls               1630 non-null   object                           \n 52  media_files              1630 non-null   object                           \n 53  media_types              1630 non-null   object                           \n 54  media_alt_texts          75 non-null     object                           \n 55  mentioned_names          2966 non-null   object                           \n 56  mentioned_ids            2567 non-null   object                           \n 57  hashtags                 166 non-null    object                           \n 58  intervention_type        0 non-null      float64                          \n 59  intervention_text        0 non-null      float64                          \n 60  intervention_url         0 non-null      float64                          \n 61  country                  7035 non-null   object                           \n 62  date                     7035 non-null   datetime64[ns, America/Sao_Paulo]\ndtypes: datetime64[ns, America/Sao_Paulo](1), float64(20), int64(1), object(41)\nmemory usage: 3.4+ MB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\nyoutu.be                       96\ntwcm.me                        56\nask.fm                         24\nyoutube.com                    18\notempo.com.br                  16\ngoogle.com.br                  13\nmoi.st                         11\nt.me                           10\ninstagram.com                   6\njornaldacidadeonline.com.br     6\nbit.ly                          6\n24.media.tumblr.com             5\nem.com.br                       5\nitatiaia.com.br                 5\ng1.globo.com                    5\nveja.abril.com.br               4\nbrasilsemmedo.com               4\n25.media.tumblr.com             4\nphelipe.com.br                  4\nlivrariadonikolas.com           4\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\n28000                       17\nvirabrasil                  12\nptnuncamais                  7\nforakalil                    6\nb28                          5\nreagebh                      5\nforamaia                     4\ndevolveodinheirojanones      4\ngobolsonaromundial           3\nmpnofelipeneto               3\nbh                           3\nbelohorizonte                3\nderretefelipeneto            3\nfamiliascontrafelipeneto     3\npaz                          3\nbolsonaro2022                3\ng1                           3\nfechadocombolsonaro          3\ndeixaopovotrabalhar          2\nnikolasnopânico              2\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\njairbolsonaro      208\nfelipeneto         143\n_portiinho         104\namanddok            99\nbrunoenglerdm       77\nbolsonarosp         61\nlorena_rcp          59\nanamarciaac         56\nlulaoficial         54\nclaramurta          52\ntaoquei1            51\nalexandrekalil      47\ndanilogentili       43\nandrejanonesadv     40\nanaclara_ah         39\ndededumontt         37\nbrendiinhasc        36\nnikolas_dm          35\nbuenoosophia        34\nfernandarian        33\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Portuguese\nnlp = spacy.load(\"pt_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\npra           1013\nbh             338\nbrasil         236\nbolsonaro      234\nlula           220\ndia            213\nesquerda       206\nhoje           196\ntá             194\ngente          181\nnao            177\npessoas        163\npresidente     150\ncara           149\nkalil          145\ndeus           138\nvei            126\npro            125\nmundo          122\nverdade        116\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n21    623\n19    592\n20    533\n18    524\n14    504\n13    488\n22    484\n12    469\n17    463\n16    406\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for iPhone        5013\nTwitter Web Client        1487\nTwitter Web App            283\nTwitter for Android        183\nTwitcom - Comunidades       58\nTwitCasting                 11\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\ntopic_model.get_topic(3)\n\n\n[('brasil', 0.19985177984675825),\n ('brasileiro', 0.03786825795393285),\n ('pra', 0.024848651813876486),\n ('brasileira', 0.02236137632133238),\n ('the', 0.020096349259891528),\n ('presidente', 0.020035211711413783),\n ('liberdade', 0.01982738009486622),\n ('and', 0.019483289697706078),\n ('in', 0.018679327283578916),\n ('bolsonaro', 0.018377388615442664)]\n\n\n\n\nCode\n# selection of topics\ntopics = [3]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @nikolas_dm, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 7035 tweets de @nikolas_dm, alrededor de 3282 hablan sobre temas de género, es decir, cerca del 46.65%\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/familiaecuador.html",
    "href": "pages/familiaecuador.html",
    "title": "Análisis de tweets de @_FamiliaEcuador",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2018-07-28 hasta 2023-03-20\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2716 entries, 196700 to 199415\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                            \n---  ------                   --------------  -----                            \n 0   query                    2716 non-null   object                           \n 1   id                       2716 non-null   float64                          \n 2   timestamp_utc            2716 non-null   int64                            \n 3   local_time               2716 non-null   object                           \n 4   user_screen_name         2716 non-null   object                           \n 5   text                     2716 non-null   object                           \n 6   possibly_sensitive       1663 non-null   object                           \n 7   retweet_count            2716 non-null   float64                          \n 8   like_count               2716 non-null   float64                          \n 9   reply_count              2716 non-null   float64                          \n 10  impression_count         46 non-null     object                           \n 11  lang                     2716 non-null   object                           \n 12  to_username              1199 non-null   object                           \n 13  to_userid                1199 non-null   float64                          \n 14  to_tweetid               1187 non-null   float64                          \n 15  source_name              2716 non-null   object                           \n 16  source_url               2716 non-null   object                           \n 17  user_location            2716 non-null   object                           \n 18  lat                      0 non-null      object                           \n 19  lng                      0 non-null      object                           \n 20  user_id                  2716 non-null   object                           \n 21  user_name                2716 non-null   object                           \n 22  user_verified            2716 non-null   float64                          \n 23  user_description         2716 non-null   object                           \n 24  user_url                 2716 non-null   object                           \n 25  user_image               2716 non-null   object                           \n 26  user_tweets              2716 non-null   object                           \n 27  user_followers           2716 non-null   float64                          \n 28  user_friends             2716 non-null   object                           \n 29  user_likes               2716 non-null   float64                          \n 30  user_lists               2716 non-null   float64                          \n 31  user_created_at          2716 non-null   object                           \n 32  user_timestamp_utc       2716 non-null   float64                          \n 33  collected_via            2716 non-null   object                           \n 34  match_query              2716 non-null   float64                          \n 35  retweeted_id             0 non-null      float64                          \n 36  retweeted_user           0 non-null      float64                          \n 37  retweeted_user_id        0 non-null      float64                          \n 38  retweeted_timestamp_utc  0 non-null      object                           \n 39  quoted_id                412 non-null    object                           \n 40  quoted_user              412 non-null    object                           \n 41  quoted_user_id           412 non-null    float64                          \n 42  quoted_timestamp_utc     412 non-null    float64                          \n 43  collection_time          2716 non-null   object                           \n 44  url                      2716 non-null   object                           \n 45  place_country_code       31 non-null     object                           \n 46  place_name               31 non-null     object                           \n 47  place_type               31 non-null     object                           \n 48  place_coordinates        31 non-null     object                           \n 49  links                    478 non-null    object                           \n 50  domains                  478 non-null    object                           \n 51  media_urls               1644 non-null   object                           \n 52  media_files              1644 non-null   object                           \n 53  media_types              1644 non-null   object                           \n 54  media_alt_texts          239 non-null    object                           \n 55  mentioned_names          1946 non-null   object                           \n 56  mentioned_ids            1904 non-null   object                           \n 57  hashtags                 1630 non-null   object                           \n 58  intervention_type        0 non-null      float64                          \n 59  intervention_text        0 non-null      float64                          \n 60  intervention_url         0 non-null      float64                          \n 61  country                  2716 non-null   object                           \n 62  date                     2716 non-null   datetime64[ns, America/Guayaquil]\ndtypes: datetime64[ns, America/Guayaquil](1), float64(20), int64(1), object(41)\nmemory usage: 1.3+ MB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\nyoutu.be                           63\nbit.ly                             43\nfacebook.com                       30\ninstagram.com                      25\nyoutube.com                        19\naciprensa.com                      19\neluniverso.com                     18\necuadorporlafamilia.org            14\narquidiocesisdeguayaquil.org.ec    11\ncitizengo.org                      11\nfamiliaecuador.org                 10\ntwitter.com                         9\nopen.spotify.com                    8\nliveaction.org                      6\nfoxnews.com                         6\nexpreso.ec                          5\npscp.tv                             5\nbuff.ly                             5\ndrive.google.com                    5\nforms.gle                           4\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\necuadoresprovida           307\nsalvemoslas2vidas          242\necuador                    176\nprovida                    118\nescudero                    89\nabortoporviolacion          78\necuadorporlafamilia         70\naborto                      63\ncoip                        58\nasambleistaqueserespeta     57\nleyabortistano              56\nmarthavillafuerte           46\nescudera                    44\nvotoprovida2021             42\nescuderos                   40\nconabortonotevoto           37\nprolife                     36\nchantajehumanitario         35\nmentirasverdes              29\njuntosporlafamilia          29\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nasambleaecuador    332\nhectoryepezm       145\njusticiaan         134\netorrescobo        132\nlenin              120\nlourdescuestao     105\namishijoseduco      88\nagustinlaje         77\namparo_medina       71\necuadorprovida      71\ncesarrohon          61\nlacristifranco      57\njulietasagnay       51\ngomezrobertoa       51\neluniversocom       49\nmarthaceciliavl     47\ncrisvalverdej       46\npolyugarteg         44\nviviana_bonilla     41\ncorteconstecu       38\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Spanish\nnlp = spacy.load(\"es_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\nvida                562\naborto              378\nfamilia             354\necuadoresprovida    316\ngracias             305\necuador             299\nprovida             218\napoyo               152\nmujeres             139\nviolación           131\nniños               120\nnacer               119\nhijos               119\nconcepción          118\ncausa               116\nley                 115\nmujer               112\nescudero            112\nthe                 108\nvoz                 108\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n12    206\n15    193\n09    183\n08    175\n10    175\n16    173\n13    172\n11    168\n14    156\n17    149\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for Android    2398\nTwitter Web App         151\nTwitter Web Client      120\nTwitter for iPhone       17\nInstagram                17\nTweetDeck                13\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# selection of topics\ntopics = [0, 2, 3]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @_FamiliaEcuador, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 2716 tweets de @_FamiliaEcuador, 1945 hablan sobre estos temas\nAlrededor del 71.61% de los tweets de @_FamiliaEcuador hablan sobre estos temas\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/unidosxlavidaco.html",
    "href": "pages/unidosxlavidaco.html",
    "title": "Análisis de tweets de @UnidosxlaVidaCo",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2011-06-14 hasta 2023-03-01\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 7830 entries, 171420 to 179249\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                         \n---  ------                   --------------  -----                         \n 0   query                    7830 non-null   object                        \n 1   id                       7830 non-null   float64                       \n 2   timestamp_utc            7830 non-null   int64                         \n 3   local_time               7830 non-null   object                        \n 4   user_screen_name         7830 non-null   object                        \n 5   text                     7830 non-null   object                        \n 6   possibly_sensitive       4135 non-null   object                        \n 7   retweet_count            7830 non-null   float64                       \n 8   like_count               7830 non-null   float64                       \n 9   reply_count              7830 non-null   float64                       \n 10  impression_count         16 non-null     object                        \n 11  lang                     7830 non-null   object                        \n 12  to_username              1568 non-null   object                        \n 13  to_userid                1568 non-null   float64                       \n 14  to_tweetid               1281 non-null   float64                       \n 15  source_name              7830 non-null   object                        \n 16  source_url               7830 non-null   object                        \n 17  user_location            7830 non-null   object                        \n 18  lat                      4 non-null      object                        \n 19  lng                      4 non-null      object                        \n 20  user_id                  7830 non-null   object                        \n 21  user_name                7830 non-null   object                        \n 22  user_verified            7830 non-null   float64                       \n 23  user_description         7830 non-null   object                        \n 24  user_url                 7830 non-null   object                        \n 25  user_image               7830 non-null   object                        \n 26  user_tweets              7830 non-null   object                        \n 27  user_followers           7830 non-null   float64                       \n 28  user_friends             7830 non-null   object                        \n 29  user_likes               7830 non-null   float64                       \n 30  user_lists               7830 non-null   float64                       \n 31  user_created_at          7830 non-null   object                        \n 32  user_timestamp_utc       7830 non-null   float64                       \n 33  collected_via            7830 non-null   object                        \n 34  match_query              7830 non-null   float64                       \n 35  retweeted_id             0 non-null      float64                       \n 36  retweeted_user           0 non-null      float64                       \n 37  retweeted_user_id        0 non-null      float64                       \n 38  retweeted_timestamp_utc  0 non-null      object                        \n 39  quoted_id                293 non-null    object                        \n 40  quoted_user              293 non-null    object                        \n 41  quoted_user_id           293 non-null    float64                       \n 42  quoted_timestamp_utc     293 non-null    float64                       \n 43  collection_time          7830 non-null   object                        \n 44  url                      7830 non-null   object                        \n 45  place_country_code       265 non-null    object                        \n 46  place_name               265 non-null    object                        \n 47  place_type               265 non-null    object                        \n 48  place_coordinates        265 non-null    object                        \n 49  links                    2904 non-null   object                        \n 50  domains                  2904 non-null   object                        \n 51  media_urls               1533 non-null   object                        \n 52  media_files              1533 non-null   object                        \n 53  media_types              1533 non-null   object                        \n 54  media_alt_texts          47 non-null     object                        \n 55  mentioned_names          2767 non-null   object                        \n 56  mentioned_ids            2613 non-null   object                        \n 57  hashtags                 4969 non-null   object                        \n 58  intervention_type        0 non-null      float64                       \n 59  intervention_text        0 non-null      float64                       \n 60  intervention_url         0 non-null      float64                       \n 61  country                  7830 non-null   object                        \n 62  date                     7830 non-null   datetime64[ns, America/Bogota]\ndtypes: datetime64[ns, America/Bogota](1), float64(20), int64(1), object(41)\nmemory usage: 3.8+ MB\n\n\n\n\nDatos\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\nquery\nid\ntimestamp_utc\nlocal_time\nuser_screen_name\ntext\npossibly_sensitive\nretweet_count\nlike_count\nreply_count\n...\nmedia_types\nmedia_alt_texts\nmentioned_names\nmentioned_ids\nhashtags\nintervention_type\nintervention_text\nintervention_url\ncountry\ndate\n\n\n\n\n171420\nfrom:UnidosxlaVidaCo\n1.630982e+18\n1677691376\n2023-03-01T17:22:56\nUnidosxlaVidaCo\n#QuienEsBeatriz y porque la @CorteIDH quiere c...\nNaN\n8.0\n7.0\n0.0\n...\nNaN\nNaN\ncorteidh\n190706828\nquienesbeatriz\nNaN\nNaN\nNaN\nColombia\n2023-03-01 12:22:56-05:00\n\n\n171421\nfrom:UnidosxlaVidaCo\n1.630982e+18\n1677691337\n2023-03-01T17:22:17\nUnidosxlaVidaCo\nUna nueva intervención de la @CorteIDH para im...\nNaN\n6.0\n4.0\n0.0\n...\nNaN\nNaN\ncorteidh\n190706828\nquienesbeatriz\nNaN\nNaN\nNaN\nColombia\n2023-03-01 12:22:17-05:00\n\n\n171422\nfrom:UnidosxlaVidaCo\n1.630981e+18\n1677691259\n2023-03-01T17:20:59\nUnidosxlaVidaCo\n#QuienEsBeatriz « LideresXlaVida: Beatriz Vs. ...\nNaN\n6.0\n6.0\n0.0\n...\nphoto\nNaN\nNaN\nNaN\nquienesbeatriz\nNaN\nNaN\nNaN\nColombia\n2023-03-01 12:20:59-05:00\n\n\n\n\n3 rows × 63 columns\n\n\n\n\n\nDominios\nLista del top 20 de dominios mencionados en los tweets y su frecuencia:\n\n\nCode\n# count items on column\ndomains = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains.nlargest(20)\n\ntop_domains\n\n\ndomains\nfb.me                     1231\nbit.ly                     242\nunidosporlavida.com        193\nfacebook.com               171\ninstagram.com              125\nsumall.com                  98\nyoutube.com                 68\nlifenews.com                40\ncitizengo.org               36\nyoutu.be                    33\n20ft.net                    33\naciprensa.com               19\nvotocatolico.co             18\nactuall.com                 15\nshar.es                     15\nliveactionnews.org          15\ntwitter.com                 12\nes.gaudiumpress.org         12\nreligionenlibertad.com      10\nrazonmasfe.com               8\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\nsialavida                647\naborto                   416\n9marchaxlavida           373\nnoalaborto               325\ncolombiaesprovida        295\neutanasia                157\nprocuradorordóñez        139\nsialprocurador           138\nyosoyprovida             135\nsoyprovida               108\nnegocio                  106\nrepost                   100\ntodavidaimporta          100\nelijolas2vidas            98\ncolombia                  93\neutanasiano               91\nabortocero                91\nfiestaxlavida             91\n4mayo7marchaporlavida     89\ncaravanaporlavida         88\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nmarceposada        196\ncolombiaprovida    194\ncconstitucional    176\nmonicaroa          173\nsialprocurador     106\nunidosxlavidaco    105\nnoticiasrcn         83\n7marcofidelr        62\namadarosa           59\nreferendoxvida      51\ncolombiaderecha     49\nprofamiliacol       48\noea_oficial         47\ncomisionprimera     42\ncamaracolombia      40\nlam_vero            36\nwradiocolombia      35\nunidosxlavida       35\nyosoyprovida        34\naciprensa           32\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Número de likes en el tiempo',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Spanish\nnlp = spacy.load(\"es_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\nvida                 2070\naborto               1097\ncolombia              719\nsialavida             661\ncolombiaesprovida     437\nmayo                  390\nq                     388\nnoalaborto            370\neutanasia             323\nderecho               323\ngracias               309\nprovida               308\nmuerte                268\nfeliz                 268\nd                     263\nvoz                   250\nmujer                 222\nfamilia               210\nmujeres               204\nconcepción            191\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n11    786\n10    737\n12    677\n09    622\n14    525\n13    519\n08    519\n07    448\n19    426\n15    403\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for iPhone             2031\nTwitter Web App                1706\nTwitter Web Client             1487\nFacebook                       1468\nTwitter for Android             412\nMobile Web                      163\nTweetDeck                       133\nerased88075                     131\nTwitter for Websites            124\nInstagram                        99\nUberSocial for iPhone            22\nMobile Web (M2)                  12\niOS                              11\nTwitter for Android Tablets      10\nTwitter for Mac                   7\nTweeet! on iOS                    4\nHootsuite Inc.                    3\nBuffer                            3\nHootsuite                         2\nTwibbon                           1\nPeriscope                         1\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# selection of topics\ntopics = [0, 1, 2]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @UnidosxlaVidaCo, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 7830 tweets de @UnidosxlaVidaCo, alrededor de 5573 hablan sobre temas de género, es decir, cerca del 71.17%\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/mamelafialloflo.html",
    "href": "pages/mamelafialloflo.html",
    "title": "Análisis de tweets de @MamelaFialloFlo",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2011-06-15 hasta 2023-03-01\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 23687 entries, 21498 to 45184\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                            \n---  ------                   --------------  -----                            \n 0   query                    23687 non-null  object                           \n 1   id                       23687 non-null  float64                          \n 2   timestamp_utc            23687 non-null  int64                            \n 3   local_time               23687 non-null  object                           \n 4   user_screen_name         23687 non-null  object                           \n 5   text                     23687 non-null  object                           \n 6   possibly_sensitive       4337 non-null   object                           \n 7   retweet_count            23687 non-null  float64                          \n 8   like_count               23687 non-null  float64                          \n 9   reply_count              23687 non-null  float64                          \n 10  impression_count         1985 non-null   object                           \n 11  lang                     23687 non-null  object                           \n 12  to_username              16705 non-null  object                           \n 13  to_userid                16705 non-null  float64                          \n 14  to_tweetid               16687 non-null  float64                          \n 15  source_name              23687 non-null  object                           \n 16  source_url               23687 non-null  object                           \n 17  user_location            0 non-null      object                           \n 18  lat                      0 non-null      object                           \n 19  lng                      0 non-null      object                           \n 20  user_id                  23687 non-null  object                           \n 21  user_name                23687 non-null  object                           \n 22  user_verified            23687 non-null  float64                          \n 23  user_description         23687 non-null  object                           \n 24  user_url                 0 non-null      object                           \n 25  user_image               23687 non-null  object                           \n 26  user_tweets              23687 non-null  object                           \n 27  user_followers           23687 non-null  float64                          \n 28  user_friends             23687 non-null  object                           \n 29  user_likes               23687 non-null  float64                          \n 30  user_lists               23687 non-null  float64                          \n 31  user_created_at          23687 non-null  object                           \n 32  user_timestamp_utc       23687 non-null  float64                          \n 33  collected_via            23687 non-null  object                           \n 34  match_query              23687 non-null  float64                          \n 35  retweeted_id             0 non-null      float64                          \n 36  retweeted_user           0 non-null      float64                          \n 37  retweeted_user_id        0 non-null      float64                          \n 38  retweeted_timestamp_utc  0 non-null      object                           \n 39  quoted_id                2119 non-null   object                           \n 40  quoted_user              2119 non-null   object                           \n 41  quoted_user_id           2119 non-null   float64                          \n 42  quoted_timestamp_utc     2119 non-null   float64                          \n 43  collection_time          23687 non-null  object                           \n 44  url                      23687 non-null  object                           \n 45  place_country_code       58 non-null     object                           \n 46  place_name               58 non-null     object                           \n 47  place_type               58 non-null     object                           \n 48  place_coordinates        58 non-null     object                           \n 49  links                    1626 non-null   object                           \n 50  domains                  1626 non-null   object                           \n 51  media_urls               3806 non-null   object                           \n 52  media_files              3806 non-null   object                           \n 53  media_types              3806 non-null   object                           \n 54  media_alt_texts          361 non-null    object                           \n 55  mentioned_names          17305 non-null  object                           \n 56  mentioned_ids            16211 non-null  object                           \n 57  hashtags                 2499 non-null   object                           \n 58  intervention_type        0 non-null      float64                          \n 59  intervention_text        0 non-null      float64                          \n 60  intervention_url         0 non-null      float64                          \n 61  country                  23687 non-null  object                           \n 62  date                     23687 non-null  datetime64[ns, America/Guayaquil]\ndtypes: datetime64[ns, America/Guayaquil](1), float64(20), int64(1), object(41)\nmemory usage: 11.6+ MB\n\n\n\n\nDatos\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\nquery\nid\ntimestamp_utc\nlocal_time\nuser_screen_name\ntext\npossibly_sensitive\nretweet_count\nlike_count\nreply_count\n...\nmedia_types\nmedia_alt_texts\nmentioned_names\nmentioned_ids\nhashtags\nintervention_type\nintervention_text\nintervention_url\ncountry\ndate\n\n\n\n\n21498\nfrom:MamelaFialloFlo\n1.638177e+18\n1679406841\n2023-03-21T13:54:01\nMamelaFialloFlo\nHoy en Ciudad de México 🇲🇽se presenta España l...\n0.0\n25.0\n78.0\n4.0\n...\nphoto|photo\n|\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nEcuador\n2023-03-21 08:54:01-05:00\n\n\n21499\nfrom:MamelaFialloFlo\n1.637879e+18\n1679335694\n2023-03-20T18:08:14\nMamelaFialloFlo\nNo podemos defender a la mujer si le decimos a...\nNaN\n79.0\n314.0\n17.0\n...\nvideo\nNaN\nNaN\nNaN\nlgbt\nNaN\nNaN\nNaN\nEcuador\n2023-03-20 13:08:14-05:00\n\n\n21500\nfrom:MamelaFialloFlo\n1.637875e+18\n1679334914\n2023-03-20T17:55:14\nMamelaFialloFlo\n@Xileone Gracias por destacarlo.\nNaN\n0.0\n9.0\n0.0\n...\nNaN\nNaN\nxileone\n65372761\nNaN\nNaN\nNaN\nNaN\nEcuador\n2023-03-20 12:55:14-05:00\n\n\n\n\n3 rows × 63 columns\n\n\n\n\n\nDominios\nLista del top 20 de dominios mencionados en los tweets y su frecuencia:\n\n\nCode\n# count items on column\ndomains = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains.nlargest(20)\n\ntop_domains\n\n\ndomains\npanampost.com                321\nes.panampost.com             165\nyoutube.com                  105\nyoutu.be                      85\ntwitter.com                   52\nbit.ly                        43\ninstagram.com                 37\ngaceta.es                     32\nfacebook.com                  21\nbuff.ly                       20\npublichealth.lacounty.gov     19\neluniverso.com                18\namazon.com                    14\nabc.es                        13\nlozierinstitute.org           13\nvatican.va                    10\namp.milenio.com                9\nlifenews.com                   9\nlibrary.brown.edu              9\nbbc.com                        9\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\nleyabortistano              243\nfemeninasífeministano        87\ncoronavirus                  87\nsalvemoslasdosvidas          64\nblacklivesmatter             64\necuadoresprovida             47\nvetopresidencial             42\ntiraníasanitaria             41\nprovida                      33\nleydelviolador               29\nsíalavida                    28\nnoalaborto                   27\nnohablesenminombre           26\nabortolegal                  26\nguateesvida                  24\ndatomatarelato               24\nlaviolencianotienegénero     23\ncovid19                      22\n8m                           21\njusticiaporlucio             19\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nagustinlaje        330\nlassoguillermo     324\nmamelafialloflo    271\npanampost_es       239\njairbolsonaro      210\netorrescobo        203\nfelipeleon88       177\nrealdonaldtrump    171\nxileone            138\npjavieror          135\nvox_es             131\nasambleaecuador    129\nsimpliciterpaco    127\nfundlibre          118\npmunoziturrieta    110\ngloriaalvarez85    109\nfreityt            109\njmilei              98\navelinaponceg       97\npontifex_es         95\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Número de likes en el tiempo',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Spanish\nnlp = spacy.load(\"es_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\ngracias       2344\nmujer         1729\nvida          1548\nmujeres       1394\naborto        1381\nfeminismo     1167\nlibertad       924\nmatar          881\nderecho        701\nthe            666\necuador        657\nhombre         630\nquieren        617\nfeministas     614\nizquierda      603\nmadre          586\npersonas       549\ndios           529\nviolencia      528\nhombres        528\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n08    1665\n09    1659\n10    1573\n22    1571\n07    1522\n11    1338\n23    1312\n21    1302\n19    1268\n12    1237\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for iPhone     16202\nTwitter Web App         7273\nTwitter Web Client       165\nTwitter for Android       47\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# selection of topics\ntopics = [0]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @MamelaFialloFlo, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 23687 tweets de @MamelaFialloFlo, alrededor de 8297 hablan sobre temas de género, es decir, cerca del 35.03%\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "notebooks/0.0-collect-tweets.html",
    "href": "notebooks/0.0-collect-tweets.html",
    "title": "Imports",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport capir_transfronteriza2_2023.data.load as load\n\n\nLoad data\n\ndata_raw = load.data_raw\ndata_minet = load.data_minet\nusers = load.users\n\n\n\nRead data\n\n# Read csv file as dataframe\ndf = pd.read_csv(users)\n\n# Show dataframe\ndf\n\n\n\n\n\n\n\n\nnombre\nusername\npais\n\n\n\n\n0\nNikolas Ferreira\nnikolas_dm\nBrasil\n\n\n1\nPastor Silas Malafaia\nPastorMalafaia\nBrasil\n\n\n2\nBrasil Sem Aborto\nbrasilsemaborto\nBrasil\n\n\n3\nMisión Paz\nmisionpaz_\nColombia\n\n\n4\nMaría Fernanda Cabal\nMariaFdaCabal\nColombia\n\n\n5\nUnidos por la Vida\nUnidosxlaVidaCo\nColombia\n\n\n6\nMamela Fiallo Flor\nMamelaFialloFlo\nEcuador\n\n\n7\nEsteban Torres Cobo\netorrescobo\nEcuador\n\n\n8\nFamilia Ecuador\n_FamiliaEcuador\nEcuador\n\n\n9\nRoy Santos\nRoySantosC\nHonduras\n\n\n10\nTomás Zambrano\nTommyZambranoM\nHonduras\n\n\n11\nLuis Felipe Faraj\nluisffaraj\nHonduras\n\n\n\n\n\n\n\n\n# Convert column values to list\nlist_users = df['username'].to_list()\n\n# Show list\nlist_users\n\n['nikolas_dm',\n 'PastorMalafaia',\n 'brasilsemaborto',\n 'misionpaz_',\n 'MariaFdaCabal',\n 'UnidosxlaVidaCo',\n 'MamelaFialloFlo',\n 'etorrescobo',\n '_FamiliaEcuador',\n 'RoySantosC',\n 'TommyZambranoM',\n 'luisffaraj']\n\n\n\n\nCollect tweets with minet\n\n\"\"\"\nExecute a shell command to scrape data from twitter, \ncollect all the tweets from a user and redirect the \noutput to a csv file\n\"\"\"\nfor i in range(len(list_users)):\n    os.system(f\"minet twitter scrape tweets 'from:{list_users[i]}' &gt; {data_minet}/{list_users[i]}.csv\")\n\nSearching for \"from:nikolas_dm\"               \nFailed attempt because of following exception:                  \nminet.twitter.exceptions.TwitterPublicAPIRateLimitError\nWill wait for 0.454 seconds before attempting again.\n\nCollecting tweet: 7035 tweets [03:51, 30.37 tweets/s, queries=1]\nSearching for \"from:PastorMalafaia\"           ]\nFailed attempt because of following exception:                  \nminet.twitter.exceptions.TwitterPublicAPIRateLimitError\nWill wait for 0.679 seconds before attempting again.\n\nFailed attempt because of following exception:                   \nminet.twitter.exceptions.TwitterPublicAPIRateLimitError\nWill wait for 0.271 seconds before attempting again.\n\nCollecting tweet: 10640 tweets [06:26, 25.78 tweets/s, queries=1]"
  },
  {
    "objectID": "notebooks/3.2-clusters-colombia.html",
    "href": "notebooks/3.2-clusters-colombia.html",
    "title": "Imports",
    "section": "",
    "text": "# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport capir_transfronteriza2_2023.data.load as load\n\n# Topic modeling\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom umap import UMAP\nfrom sklearn.cluster import KMeans\nfrom hdbscan import HDBSCAN\n\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n\n\n\nLoad data\n\ndata_processed = load.data_processed\nassets = load.assets\n\n\n\nRead data from Colombia\n\ndata = pd.read_csv(f\"{data_processed}/colombia.csv\")\n\nprint(data.shape)\n\ndata.head(3)\n\n(57477, 4)\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\n\n\n\n\n0\nMariaFdaCabal\nColombia\nPeligro que personas señaladas por Rosa Blanca...\nPeligro que personas señaladas por Rosa Blanca...\n\n\n1\nMariaFdaCabal\nColombia\n¡Al fin! El gobierno de la potencia mundial de...\n¡Al fin! El gobierno de la potencia mundial de...\n\n\n2\nMariaFdaCabal\nColombia\nQué buen artículo. \"Sandeces democratíceras\". ...\nQué buen artículo. \"Sandeces democratíceras\".\n\n\n\n\n\n\n\n\n\nProcess data\n\ncountry = data[data['text_clean'].apply(lambda x: isinstance(x, (str, bytes)))]\n\ncountry['text_cluster'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\nprint(country.shape)\n\n(54974, 5)\n\n\n/tmp/ipykernel_58249/1630503326.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country['text_cluster'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\n\n\n\nCreate embeddings\n\n# Load model to generate embeddings\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n# Process data with encoder\nvecs = sbert_model.encode(country[\"text_clean\"].values)\n\n\n# Apply the dimensionality reduction process.\n\n# Scaler to normalize data\nscaler = StandardScaler()\n# PCA to reduce multicollinearity and noise\npca_ = PCA(0.9, random_state=5)\n# UMAP to reduce dimensionality to two components\numap_ = UMAP(n_components=2,\n             random_state=5,\n             metric=\"cosine\",\n             n_neighbors=50,\n             min_dist=0.1)\n\nvecs_sc = scaler.fit_transform(vecs)\nvecs_pca = pca_.fit_transform(vecs_sc)\nvecs_umap = umap_.fit_transform(vecs_pca)\n\n\n# Assign components to dataset\ncountry.loc[:,\"COMP_1\"] = vecs_umap[:,0]\ncountry.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n# Show data\ncountry.head()\n\n/tmp/ipykernel_58249/4075971638.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country.loc[:,\"COMP_1\"] = vecs_umap[:,0]\n/tmp/ipykernel_58249/4075971638.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\ntext_cluster\nCOMP_1\nCOMP_2\n\n\n\n\n0\nMariaFdaCabal\nColombia\nPeligro que personas señaladas por Rosa Blanca...\nPeligro que personas señaladas por Rosa Blanca...\nPeligro que personas señaladas por Rosa Blanca...\n5.661910\n2.847879\n\n\n1\nMariaFdaCabal\nColombia\n¡Al fin! El gobierno de la potencia mundial de...\n¡Al fin! El gobierno de la potencia mundial de...\n¡Al fin! El gobierno de la potencia mundial de...\n4.021524\n2.784703\n\n\n2\nMariaFdaCabal\nColombia\nQué buen artículo. \"Sandeces democratíceras\". ...\nQué buen artículo. \"Sandeces democratíceras\".\nQué buen artículo. \"Sandeces democratíceras\". ...\n4.688560\n3.120051\n\n\n3\nMariaFdaCabal\nColombia\nLos izquierdópatas en Colombia aplauden el lem...\nLos izquierdópatas en Colombia aplauden el lem...\nLos izquierdópatas en Colombia aplauden el lem...\n6.964063\n4.052229\n\n\n4\nMariaFdaCabal\nColombia\nDenuncian llegada de presuntos colectivos vene...\nDenuncian llegada de presuntos colectivos vene...\nDenuncian llegada de presuntos colectivos vene...\n4.018866\n4.329624\n\n\n\n\n\n\n\n\n# Visualize the distribution of embeddings with a hover to see the tweets.\nfig = px.scatter(\n    country,\n    x=\"COMP_1\",\n    y=\"COMP_2\",\n    hover_data=[\"user_screen_name\", \"text_clean\"])\nfig.write_html(f\"{assets}/colombia/colombia.html\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nClustering\n\n# Apply the HDBSCAN algorithm to extract the clusters.\n\nmc_size = 100\nm_samples = 1\n\ndb_model = HDBSCAN(min_cluster_size=mc_size,\n                   min_samples=m_samples,\n                   metric=\"euclidean\", \n                   )\ndb_model.fit(country.loc[:,[\"COMP_1\", \"COMP_2\"]])\ncountry.loc[:, \"CLUSTER\"] = db_model.labels_\ncountry.loc[:, \"PROBA\"] = db_model.probabilities_\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n/tmp/ipykernel_58249/2719550865.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/tmp/ipykernel_58249/2719550865.py:12: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Visualize clusters\nplt.figure(figsize=(12,9))\nplt.scatter(vecs_umap[:,0], vecs_umap[:,1], alpha=0.7, c=db_model.labels_, s=10)\nplt.suptitle(f\"Publicaciones de antiderechos en Colombia\\nMin cluster size: {mc_size} | Min samples: {m_samples}\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/colombia/colombia_clusters\")\nplt.show()\n\n\n\n\n\n# Extract the centroid of each cluster\n\ncl_centroids = {}\n\nfor i in sorted(country[\"CLUSTER\"].unique()):\n\n  cl_centroids[i] = country[country[\"CLUSTER\"] == i][[\"COMP_1\", \"COMP_2\"]].mean().values\n\ncentroids_array = np.array(list(cl_centroids.values()))\n\nfig, ax = plt.subplots(figsize=(12,9))\nax.scatter(vecs_umap[:,0], vecs_umap[:,1], s=10, alpha=0.7, c=db_model.labels_)\nax.scatter(centroids_array[:,0], centroids_array[:,1], marker=\"*\", c=\"red\")\nfor i in cl_centroids:\n  ax.annotate(i, cl_centroids.get(i)+np.array([0.1, 0.1]))\nplt.suptitle(f\"Centroides de {len(cl_centroids)} clusters de antiderechos en Colombia\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/colombia/colombia_centroids\", dpi=fig.dpi)\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nN = 20\n\nwith open(f\"{assets}/colombia/colombia.txt\", 'w') as f:\n  for i in cl_centroids:\n    point = cl_centroids.get(i).reshape(1,-1)\n    ix_min_dist = np.argsort(euclidean_distances(point, country.loc[:, [\"COMP_1\", \"COMP_2\"]])).flatten()[:N]\n    print(f\"\\n[Tweets más representativos del cluster {i}]\", file=f)\n    users = country.iloc[ix_min_dist, country.columns.get_loc(\"user_screen_name\")].values\n    texts = country.iloc[ix_min_dist, country.columns.get_loc(\"text_cluster\")].values\n    tweets = [[x, y] for x, y in zip(users, texts)]\n    print('\\n'.join([': '.join(map(str, inner_list)) for inner_list in tweets]), file=f)"
  },
  {
    "objectID": "notebooks/3.3-clusters-honduras.html",
    "href": "notebooks/3.3-clusters-honduras.html",
    "title": "Imports",
    "section": "",
    "text": "# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport capir_transfronteriza2_2023.data.load as load\n\n# Topic modeling\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom umap import UMAP\nfrom sklearn.cluster import KMeans\nfrom hdbscan import HDBSCAN\n\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n\n\n\nLoad data\n\ndata_processed = load.data_processed\nassets = load.assets\n\n\n\nRead data from Honduras\n\ndata = pd.read_csv(f\"{data_processed}/honduras.csv\")\n\nprint(data.shape)\n\ndata.head(3)\n\n(55733, 4)\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\n\n\n\n\n0\nTommyZambranoM\nHonduras\nLos Nacionalistas para lograr la renovación de...\nLos Nacionalistas para lograr la renovación de...\n\n\n1\nTommyZambranoM\nHonduras\n#LasÑangaradas de la Semana:\\n1) Sin comer tie...\n$HASHTAG$ de la Semana: 1) Sin comer tienen a ...\n\n\n2\nTommyZambranoM\nHonduras\nMira el futuro con Fe , llénate de esperanza y...\nMira el futuro con Fe , llénate de esperanza y...\n\n\n\n\n\n\n\n\n\nProcess data\n\ncountry = data[data['text_clean'].apply(lambda x: isinstance(x, (str, bytes)))]\n\ncountry['text_cluster'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\nprint(country.shape)\n\n(54559, 5)\n\n\n/tmp/ipykernel_62748/1630503326.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country['text_cluster'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\n\n\n\nCreate embeddings\n\n# Load model to generate embeddings\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n# Process data with encoder\nvecs = sbert_model.encode(country[\"text_clean\"].values)\n\n\n# Apply the dimensionality reduction process.\n\n# Scaler to normalize data\nscaler = StandardScaler()\n# PCA to reduce multicollinearity and noise\npca_ = PCA(0.9, random_state=5)\n# UMAP to reduce dimensionality to two components\numap_ = UMAP(n_components=2,\n             random_state=5,\n             metric=\"cosine\",\n             n_neighbors=50,\n             min_dist=0.1)\n\nvecs_sc = scaler.fit_transform(vecs)\nvecs_pca = pca_.fit_transform(vecs_sc)\nvecs_umap = umap_.fit_transform(vecs_pca)\n\n\n# Assign components to dataset\ncountry.loc[:,\"COMP_1\"] = vecs_umap[:,0]\ncountry.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n# Show data\ncountry.head()\n\n/tmp/ipykernel_62748/4075971638.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country.loc[:,\"COMP_1\"] = vecs_umap[:,0]\n/tmp/ipykernel_62748/4075971638.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\ntext_cluster\nCOMP_1\nCOMP_2\n\n\n\n\n0\nTommyZambranoM\nHonduras\nLos Nacionalistas para lograr la renovación de...\nLos Nacionalistas para lograr la renovación de...\nLos Nacionalistas para lograr la renovación de...\n5.976500\n-5.228549\n\n\n1\nTommyZambranoM\nHonduras\n#LasÑangaradas de la Semana:\\n1) Sin comer tie...\n$HASHTAG$ de la Semana: 1) Sin comer tienen a ...\n#LasÑangaradas de la Semana:1) Sin comer tiene...\n6.442870\n-5.310290\n\n\n2\nTommyZambranoM\nHonduras\nMira el futuro con Fe , llénate de esperanza y...\nMira el futuro con Fe , llénate de esperanza y...\nMira el futuro con Fe , llénate de esperanza y...\n6.845706\n-5.148116\n\n\n3\nTommyZambranoM\nHonduras\nFelicitamos a los héroes de casa en este dia ;...\nFelicitamos a los héroes de casa en este dia ;...\nFelicitamos a los héroes de casa en este dia ;...\n9.583169\n5.675184\n\n\n4\nTommyZambranoM\nHonduras\nLos Cachurecos tenemos que ser como las águila...\nLos Cachurecos tenemos que ser como las águila...\nLos Cachurecos tenemos que ser como las águila...\n9.784096\n4.484969\n\n\n\n\n\n\n\n\n# Visualize the distribution of embeddings with a hover to see the tweets.\nfig = px.scatter(\n    country,\n    x=\"COMP_1\",\n    y=\"COMP_2\",\n    hover_data=[\"user_screen_name\", \"text_clean\"])\nfig.write_html(f\"{assets}/honduras/honduras.html\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nClustering\n\n# Apply the HDBSCAN algorithm to extract the clusters.\n\nmc_size = 120\nm_samples = 1\n\ndb_model = HDBSCAN(min_cluster_size=mc_size,\n                   min_samples=m_samples,\n                   metric=\"euclidean\", \n                   )\ndb_model.fit(country.loc[:,[\"COMP_1\", \"COMP_2\"]])\ncountry.loc[:, \"CLUSTER\"] = db_model.labels_\ncountry.loc[:, \"PROBA\"] = db_model.probabilities_\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n/tmp/ipykernel_62748/1051480248.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/tmp/ipykernel_62748/1051480248.py:12: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Visualize clusters\nplt.figure(figsize=(12,9))\nplt.scatter(vecs_umap[:,0], vecs_umap[:,1], alpha=0.7, c=db_model.labels_, s=10)\nplt.suptitle(f\"Publicaciones de antiderechos en Honduras\\nMin cluster size: {mc_size} | Min samples: {m_samples}\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/honduras/honduras_clusters\")\nplt.show()\n\n\n\n\n\n# Extract the centroid of each cluster\n\ncl_centroids = {}\n\nfor i in sorted(country[\"CLUSTER\"].unique()):\n\n  cl_centroids[i] = country[country[\"CLUSTER\"] == i][[\"COMP_1\", \"COMP_2\"]].mean().values\n\ncentroids_array = np.array(list(cl_centroids.values()))\n\nfig, ax = plt.subplots(figsize=(12,9))\nax.scatter(vecs_umap[:,0], vecs_umap[:,1], s=10, alpha=0.7, c=db_model.labels_)\nax.scatter(centroids_array[:,0], centroids_array[:,1], marker=\"*\", c=\"red\")\nfor i in cl_centroids:\n  ax.annotate(i, cl_centroids.get(i)+np.array([0.1, 0.1]))\nplt.suptitle(f\"Centroides de {len(cl_centroids)} clusters de antiderechos en Honduras\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/honduras/honduras_centroids\", dpi=fig.dpi)\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nN = 20\n\nwith open(f\"{assets}/honduras/honduras.txt\", 'w') as f:\n  for i in cl_centroids:\n    point = cl_centroids.get(i).reshape(1,-1)\n    ix_min_dist = np.argsort(euclidean_distances(point, country.loc[:, [\"COMP_1\", \"COMP_2\"]])).flatten()[:N]\n    print(f\"\\n[Tweets más representativos del cluster {i}]\", file=f)\n    users = country.iloc[ix_min_dist, country.columns.get_loc(\"user_screen_name\")].values\n    texts = country.iloc[ix_min_dist, country.columns.get_loc(\"text_cluster\")].values\n    tweets = [[x, y] for x, y in zip(users, texts)]\n    print('\\n'.join([': '.join(map(str, inner_list)) for inner_list in tweets]), file=f)"
  },
  {
    "objectID": "notebooks/2.0-analyze-data.html",
    "href": "notebooks/2.0-analyze-data.html",
    "title": "Imports",
    "section": "",
    "text": "import pandas as pd\nimport preprocessor as p\nfrom emoji import demojize\nimport capir_transfronteriza2_2023.data.load as load\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nLoad data\n\ndata_processed = load.data_processed\n\n\n\nRead data\n\n# Read csv file as dataframe\ndf = pd.read_csv(f'{data_processed}/tweets.csv')\n\n# Print dataframe shape\nprint(df.shape)\n\n# Show dataframe\ndf.head(3)\n\n(200827, 62)\n\n\n\n\n\n\n\n\n\nquery\nid\ntimestamp_utc\nlocal_time\nuser_screen_name\ntext\npossibly_sensitive\nretweet_count\nlike_count\nreply_count\n...\nmedia_files\nmedia_types\nmedia_alt_texts\nmentioned_names\nmentioned_ids\nhashtags\nintervention_type\nintervention_text\nintervention_url\ncountry\n\n\n\n\n0\nfrom:TommyZambranoM\n1.638175e+18\n1679406309\n2023-03-21T13:45:09\nTommyZambranoM\nLos Nacionalistas para lograr la renovación de...\n0.0\n30.0\n117.0\n58.0\n...\n1638174913906327552_n59p6Id5p1YViXtP.mp4\nvideo\nNaN\npnh_oficial\n201589327\nlibrenuncamas\nNaN\nNaN\nNaN\nHonduras\n\n\n1\nfrom:TommyZambranoM\n1.637800e+18\n1679316989\n2023-03-20T12:56:29\nTommyZambranoM\n#LasÑangaradas de la Semana:\\n1) Sin comer tie...\n0.0\n78.0\n292.0\n178.0\n...\n1637800275900604418_Frqiq2uWwAAq6ZM.jpg\nphoto\nNaN\nNaN\nNaN\nlasñangaradas\nNaN\nNaN\nNaN\nHonduras\n\n\n2\nfrom:TommyZambranoM\n1.637795e+18\n1679315747\n2023-03-20T12:35:47\nTommyZambranoM\nMira el futuro con Fe , llénate de esperanza y...\nNaN\n12.0\n61.0\n14.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\ntommyzambrano\nNaN\nNaN\nNaN\nHonduras\n\n\n\n\n3 rows × 62 columns\n\n\n\n\n\nProcess data\n\ndf['country'].value_counts()\n\ncountry\nColombia    57741\nHonduras    56212\nBrasil      52155\nEcuador     34717\nName: count, dtype: int64\n\n\n\ncountry_list = df['country'].unique().tolist()\n\ncountries = list(filter(lambda x: isinstance(x, str), country_list))\n\ncountries\n\n['Honduras', 'Brasil', 'Ecuador', 'Colombia']\n\n\n\nfor i in range(len(countries)):\n\n    # Filter data\n    country = df[df['country'] == countries[i]]\n\n    # Remove duplicates\n    country.drop_duplicates(subset=\"text\", keep='first', inplace=True)\n\n    # Remove urls\n    p.set_options(p.OPT.URL)\n    country['text_clean'] = country['text'].apply(lambda x: p.clean(x))\n\n    # Tokenize mentions and hashtags\n    p.set_options(p.OPT.MENTION, p.OPT.HASHTAG)\n    country['text_clean'] = country['text_clean'].apply(lambda x: p.tokenize(x))\n\n    # Replace emojis with descriptions\n    country['text_clean'] = country['text_clean'].apply(lambda x: demojize(x))\n\n    # Filter columns\n    country_fil = country[['user_screen_name', 'country', 'text', 'text_clean']]\n\n    # Set up filename in lower case\n    filename = countries[i].lower()\n\n    # Print dataframe shape\n    print(f'{countries[i]}: {country_fil.shape}')\n    \n    # Save dataframe as 'csv' file\n    country_fil.to_csv(f'{data_processed}/{filename}.csv', index=False)\n\nHonduras: (55733, 4)\nBrasil: (47697, 4)\nEcuador: (34693, 4)\nColombia: (57477, 4)"
  },
  {
    "objectID": "notebooks/3.0-clusters-brasil.html",
    "href": "notebooks/3.0-clusters-brasil.html",
    "title": "Installation",
    "section": "",
    "text": "!pipenv install matplotlib\n!pipenv install plotly\n!pipenv install sentence_transformers\n!pipenv install umap-learn\n!pipenv install hdbscan\n!pipenv install joblib==1.1.0\n\nCourtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.\nInstalling sentence_transformers...\nResolving sentence_transformers...\nInstalling...\nAdding sentence_transformers to Pipfile's [packages] ...\n✔ Installation Succeededce_transformers...\n⠧ Installing sentence_transformers...\nPipfile.lock (ed3c53) out of date, updating to (8b205d)...\nLocking [packages] dependencies...\nBuilding requirements...\nResolving dependencies...\n✔ Success! Locking...\n⠦ Locking...\nLocking [dev-packages] dependencies...\nUpdated Pipfile.lock (5707ed3afeb676b8a853ffe31c724dfaba4e379b9b01e2b1348da8775f8b205d)!\nInstalling dependencies from Pipfile.lock (8b205d)...\nTo activate this project's virtualenv, run pipenv shell.\nAlternatively, run a command inside the virtualenv with pipenv run.\nCourtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.\nInstalling umap-learn...\nResolving umap-learn...\nInstalling...\nAdding umap-learn to Pipfile's [packages] ...\n✔ Installation Succeededearn...\n⠏ Installing umap-learn...\nPipfile.lock (8b205d) out of date, updating to (bd7e61)...\nLocking [packages] dependencies...\nBuilding requirements...\nResolving dependencies...\n✔ Success! Locking...\n⠦ Locking...\nLocking [dev-packages] dependencies...\nUpdated Pipfile.lock (f8b8398b1f93b2eabef8b0aa919d2944e3c267fe25d1dd5c535e11a481bd7e61)!\nInstalling dependencies from Pipfile.lock (bd7e61)...\nTo activate this project's virtualenv, run pipenv shell.\nAlternatively, run a command inside the virtualenv with pipenv run.\nCourtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.\nInstalling hdbscan...\nResolving hdbscan...\nInstalling...\nAdding hdbscan to Pipfile's [packages] ...\n✔ Installation Succeededn...\n⠹ Installing hdbscan...\nPipfile.lock (bd7e61) out of date, updating to (b3ac28)...\nLocking [packages] dependencies...\nBuilding requirements...\nResolving dependencies...\n✔ Success! Locking...\n⠹ Locking...\nLocking [dev-packages] dependencies...\nUpdated Pipfile.lock (3216dac2a46727855ded1d965cca2bdbdcd584a5f95685a889baa0faa9b3ac28)!\nInstalling dependencies from Pipfile.lock (b3ac28)...\nTo activate this project's virtualenv, run pipenv shell.\nAlternatively, run a command inside the virtualenv with pipenv run.\nCourtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.\nInstalling joblib==1.1.0...\nResolving joblib==1.1.0...\nInstalling...\nAdding joblib to Pipfile's [packages] ...\n✔ Installation Succeeded...\n⠴ Installing joblib...\nPipfile.lock (b3ac28) out of date, updating to (4b448b)...\nLocking [packages] dependencies...\nBuilding requirements...\nResolving dependencies...\n✔ Success! Locking...\n⠙ Locking...\nLocking [dev-packages] dependencies...\nUpdated Pipfile.lock (85165b5a25e2892f003e074fd0b7ee53040705b5c30deaf855d3151d064b448b)!\nInstalling dependencies from Pipfile.lock (4b448b)...\nTo activate this project's virtualenv, run pipenv shell.\nAlternatively, run a command inside the virtualenv with pipenv run.\n\n\n\nImports\n\n# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport capir_transfronteriza2_2023.data.load as load\n\n# Topic modeling\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom umap import UMAP\nfrom sklearn.cluster import KMeans\nfrom hdbscan import HDBSCAN\n\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n/home/fxr/.local/share/virtualenvs/capir_transfronteriza2_2023-f1a4fPBO/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit()\n\n\n\n\nLoad data\n\ndata_processed = load.data_processed\nassets = load.assets\n\n\n\nRead data from Brazil\n\ndata = pd.read_csv(f\"{data_processed}/brasil.csv\")\n\nprint(data.shape)\n\ndata.head(3)\n\n(47697, 4)\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\n\n\n\n\n0\nnikolas_dm\nBrasil\nParabéns pro homem que resgatou nosso orgulho ...\nParabéns pro homem que resgatou nosso orgulho ...\n\n\n1\nnikolas_dm\nBrasil\n“Efeito Nikolas” - Bom dia 👍🏻🇧🇷 https://twitte...\n“Efeito Nikolas” - Bom dia :thumbs_up_light_sk...\n\n\n2\nnikolas_dm\nBrasil\n@splucasaugusto Há esperança! Continuemos.\n$MENTION$ Há esperança! Continuemos.\n\n\n\n\n\n\n\n\n\nProcess data\n\ncountry = data[data['text_clean'].apply(lambda x: isinstance(x, (str, bytes)))]\n\ncountry['text_cluster'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\nprint(country.shape)\n\n(47560, 5)\n\n\n/tmp/ipykernel_55360/3755556193.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country['text_pre'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\n\n\n\nCreate embeddings\n\n# Load model to generate embeddings\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n# Process data with encoder\nvecs = sbert_model.encode(country[\"text_clean\"].values)\n\n\n# Apply the dimensionality reduction process.\n\n# Scaler to normalize data\nscaler = StandardScaler()\n# PCA to reduce multicollinearity and noise\npca_ = PCA(0.9, random_state=5)\n# UMAP to reduce dimensionality to two components\numap_ = UMAP(n_components=2,\n             random_state=5,\n             metric=\"cosine\",\n             n_neighbors=50,\n             min_dist=0.1)\n\nvecs_sc = scaler.fit_transform(vecs)\nvecs_pca = pca_.fit_transform(vecs_sc)\nvecs_umap = umap_.fit_transform(vecs_pca)\n\n\n# Assign components to dataset\ncountry.loc[:,\"COMP_1\"] = vecs_umap[:,0]\ncountry.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n# Show data\ncountry.head()\n\n/tmp/ipykernel_55360/4075971638.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country.loc[:,\"COMP_1\"] = vecs_umap[:,0]\n/tmp/ipykernel_55360/4075971638.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  country.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\ntext_pre\nCOMP_1\nCOMP_2\n\n\n\n\n0\nnikolas_dm\nBrasil\nParabéns pro homem que resgatou nosso orgulho ...\nParabéns pro homem que resgatou nosso orgulho ...\nParabéns pro homem que resgatou nosso orgulho ...\n7.242025\n6.089822\n\n\n1\nnikolas_dm\nBrasil\n“Efeito Nikolas” - Bom dia 👍🏻🇧🇷 https://twitte...\n“Efeito Nikolas” - Bom dia :thumbs_up_light_sk...\n“Efeito Nikolas” - Bom dia 👍🏻🇧🇷 https://twitte...\n1.333471\n10.964771\n\n\n2\nnikolas_dm\nBrasil\n@splucasaugusto Há esperança! Continuemos.\n$MENTION$ Há esperança! Continuemos.\n@splucasaugusto Há esperança! Continuemos.\n3.252815\n10.122753\n\n\n3\nnikolas_dm\nBrasil\nJá tem foca pulando do barco, o burrinho do sh...\nJá tem foca pulando do barco, o burrinho do sh...\nJá tem foca pulando do barco, o burrinho do sh...\n6.660804\n6.952069\n\n\n4\nnikolas_dm\nBrasil\nVisitei hoje Hospital universitário Ciências M...\nVisitei hoje Hospital universitário Ciências M...\nVisitei hoje Hospital universitário Ciências M...\n8.056169\n9.198310\n\n\n\n\n\n\n\n\n# Visualize the distribution of embeddings with a hover to see the tweets.\nfig = px.scatter(\n    country,\n    x=\"COMP_1\",\n    y=\"COMP_2\",\n    hover_data=[\"user_screen_name\", \"text_clean\"])\nfig.write_html(f\"{assets}/brasil/brasil.html\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nClustering\n\n# Apply the HDBSCAN algorithm to extract the clusters.\n\nmc_size = 250\nm_samples = 1\n\ndb_model = HDBSCAN(min_cluster_size=mc_size,\n                   min_samples=m_samples,\n                   metric=\"euclidean\", \n                   )\ndb_model.fit(country.loc[:,[\"COMP_1\", \"COMP_2\"]])\ncountry.loc[:, \"CLUSTER\"] = db_model.labels_\ncountry.loc[:, \"PROBA\"] = db_model.probabilities_\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n/tmp/ipykernel_55360/237107815.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/tmp/ipykernel_55360/237107815.py:12: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Visualize clusters\nplt.figure(figsize=(12,9))\nplt.scatter(vecs_umap[:,0], vecs_umap[:,1], alpha=0.7, c=db_model.labels_, s=10)\nplt.suptitle(f\"Publicaciones de antiderechos en Brasil\\nMin cluster size: {mc_size} | Min samples: {m_samples}\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/brasil/brasil_clusters\")\nplt.show()\n\n\n\n\n\n# Extract the centroid of each cluster\n\ncl_centroids = {}\n\nfor i in sorted(country[\"CLUSTER\"].unique()):\n\n  cl_centroids[i] = country[country[\"CLUSTER\"] == i][[\"COMP_1\", \"COMP_2\"]].mean().values\n\ncentroids_array = np.array(list(cl_centroids.values()))\n\nfig, ax = plt.subplots(figsize=(12,9))\nax.scatter(vecs_umap[:,0], vecs_umap[:,1], s=10, alpha=0.7, c=db_model.labels_)\nax.scatter(centroids_array[:,0], centroids_array[:,1], marker=\"*\", c=\"red\")\nfor i in cl_centroids:\n  ax.annotate(i, cl_centroids.get(i)+np.array([0.1, 0.1]))\nplt.suptitle(f\"Centroides de {len(cl_centroids)} clusters de antiderechos en Brasil\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/brasil/brasil_centroids\", dpi=fig.dpi)\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nN = 20\n\nwith open(f\"{assets}/brasil/brasil.txt\", 'w') as f:\n  for i in cl_centroids:\n    point = cl_centroids.get(i).reshape(1,-1)\n    ix_min_dist = np.argsort(euclidean_distances(point, country.loc[:, [\"COMP_1\", \"COMP_2\"]])).flatten()[:N]\n    print(f\"\\n[Tweets más representativos del cluster {i}]\", file=f)\n    users = country.iloc[ix_min_dist, country.columns.get_loc(\"user_screen_name\")].values\n    texts = country.iloc[ix_min_dist, country.columns.get_loc(\"text_cluster\")].values\n    tweets = [[x, y] for x, y in zip(users, texts)]\n    print('\\n'.join([': '.join(map(str, inner_list)) for inner_list in tweets]), file=f)"
  },
  {
    "objectID": "notebooks/1.0-process-data.html",
    "href": "notebooks/1.0-process-data.html",
    "title": "Imports",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport capir_transfronteriza2_2023.data.load as load\nimport capir_transfronteriza2_2023.data.process as process\n\n\nLoad data\n\ndata_minet = load.data_minet\ndata_processed = load.data_processed\n\n\n\nRead data\n\nfiles = process.get_csv_files(data_minet)\n\n\ndfs = []\nfor f in files:\n    df = pd.read_csv(f, low_memory=False)\n    dfs.append(df)\n\n\ndf = pd.concat(dfs)\n\nprint(df.shape)\n\ndf.head(3)\n\n(200826, 61)\n\n\n\n\n\n\n\n\n\nquery\nid\ntimestamp_utc\nlocal_time\nuser_screen_name\ntext\npossibly_sensitive\nretweet_count\nlike_count\nreply_count\n...\nmedia_urls\nmedia_files\nmedia_types\nmedia_alt_texts\nmentioned_names\nmentioned_ids\nhashtags\nintervention_type\nintervention_text\nintervention_url\n\n\n\n\n0\nfrom:TommyZambranoM\n1638174913906327552\n1679406309\n2023-03-21T13:45:09\nTommyZambranoM\nLos Nacionalistas para lograr la renovación de...\n0.0\n30\n117\n58\n...\nhttps://video.twimg.com/amplify_video/16381748...\n1638174913906327552_n59p6Id5p1YViXtP.mp4\nvideo\nNaN\npnh_oficial\n201589327\nlibrenuncamas\nNaN\nNaN\nNaN\n\n\n1\nfrom:TommyZambranoM\n1637800275900604418\n1679316989\n2023-03-20T12:56:29\nTommyZambranoM\n#LasÑangaradas de la Semana:\\n1) Sin comer tie...\n0.0\n78\n292\n178\n...\nhttps://pbs.twimg.com/media/Frqiq2uWwAAq6ZM.jpg\n1637800275900604418_Frqiq2uWwAAq6ZM.jpg\nphoto\nNaN\nNaN\nNaN\nlasñangaradas\nNaN\nNaN\nNaN\n\n\n2\nfrom:TommyZambranoM\n1637795067913023488\n1679315747\n2023-03-20T12:35:47\nTommyZambranoM\nMira el futuro con Fe , llénate de esperanza y...\nNaN\n12\n61\n14\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\ntommyzambrano\nNaN\nNaN\nNaN\n\n\n\n\n3 rows × 61 columns\n\n\n\n\n# Adding column for country\nconditions = [\n    (df['user_screen_name'] == \"nikolas_dm\") | (df['user_screen_name'] == \"PastorMalafaia\") | (df['user_screen_name'] == \"brasilsemaborto\"),\n    (df['user_screen_name'] == \"MamelaFialloFlo\") | (df['user_screen_name'] == \"etorrescobo\") | (df['user_screen_name'] == \"_FamiliaEcuador\"),\n    (df['user_screen_name'] == \"misionpaz_\") | (df['user_screen_name'] == \"MariaFdaCabal\") | (df['user_screen_name'] == \"UnidosxlaVidaCo\"),\n    (df['user_screen_name'] == \"RoySantosC\") | (df['user_screen_name'] == \"TommyZambranoM\") | (df['user_screen_name'] == \"luisffaraj\"),\n    ]\n\nvalues = ['Brasil', 'Ecuador', 'Colombia', 'Honduras']\n\ndf['country'] = np.select(conditions, values)\n\ndf.head(3)\n\n\n\n\n\n\n\n\nquery\nid\ntimestamp_utc\nlocal_time\nuser_screen_name\ntext\npossibly_sensitive\nretweet_count\nlike_count\nreply_count\n...\nmedia_files\nmedia_types\nmedia_alt_texts\nmentioned_names\nmentioned_ids\nhashtags\nintervention_type\nintervention_text\nintervention_url\ncountry\n\n\n\n\n0\nfrom:TommyZambranoM\n1638174913906327552\n1679406309\n2023-03-21T13:45:09\nTommyZambranoM\nLos Nacionalistas para lograr la renovación de...\n0.0\n30\n117\n58\n...\n1638174913906327552_n59p6Id5p1YViXtP.mp4\nvideo\nNaN\npnh_oficial\n201589327\nlibrenuncamas\nNaN\nNaN\nNaN\nHonduras\n\n\n1\nfrom:TommyZambranoM\n1637800275900604418\n1679316989\n2023-03-20T12:56:29\nTommyZambranoM\n#LasÑangaradas de la Semana:\\n1) Sin comer tie...\n0.0\n78\n292\n178\n...\n1637800275900604418_Frqiq2uWwAAq6ZM.jpg\nphoto\nNaN\nNaN\nNaN\nlasñangaradas\nNaN\nNaN\nNaN\nHonduras\n\n\n2\nfrom:TommyZambranoM\n1637795067913023488\n1679315747\n2023-03-20T12:35:47\nTommyZambranoM\nMira el futuro con Fe , llénate de esperanza y...\nNaN\n12\n61\n14\n...\nNaN\nNaN\nNaN\nNaN\nNaN\ntommyzambrano\nNaN\nNaN\nNaN\nHonduras\n\n\n\n\n3 rows × 62 columns\n\n\n\n\ndf.to_csv(f'{data_processed}/tweets.csv', index=False)"
  },
  {
    "objectID": "notebooks/3.1-clusters-ecuador.html",
    "href": "notebooks/3.1-clusters-ecuador.html",
    "title": "Imports",
    "section": "",
    "text": "# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport capir_transfronteriza2_2023.data.load as load\n\n# Topic modeling\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom umap import UMAP\nfrom sklearn.cluster import KMeans\nfrom hdbscan import HDBSCAN\n\n\nLoad data\n\ndata_processed = load.data_processed\nassets = load.assets\n\n\n\nRead data from Ecuador\n\ndata = pd.read_csv(f\"{data_processed}/ecuador.csv\")\n\nprint(data.shape)\n\ndata.head(3)\n\n(34693, 4)\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\n\n\n\n\n0\netorrescobo\nEcuador\nJuan Carlos, pero si atrás del video están ust...\nJuan Carlos, pero si atrás del video están ust...\n\n\n1\netorrescobo\nEcuador\n@ViTTO095 @AndresSeminario Se les escapó entre...\n$MENTION$ $MENTION$ Se les escapó entre chisme...\n\n\n2\netorrescobo\nEcuador\nSolidaridad con @LeninArtieda y con @ecuavisa....\nSolidaridad con $MENTION$ y con $MENTION$. Imp...\n\n\n\n\n\n\n\n\n\nProcess data\n\ncountry = data[data['text_clean'].apply(lambda x: isinstance(x, (str, bytes)))]\n\ncountry['text_cluster'] = country['text'].apply(lambda x: x.replace('\\n',''))\n\nprint(country.shape)\n\n(34573, 5)\n\n\n/tmp/ipykernel_56168/1630503326.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\nCreate embeddings\n\n# Load model to generate embeddings\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n# Process data with encoder\nvecs = sbert_model.encode(country[\"text_clean\"].values)\n\n\n# Apply the dimensionality reduction process.\n\n# Scaler to normalize data\nscaler = StandardScaler()\n# PCA to reduce multicollinearity and noise\npca_ = PCA(0.9, random_state=5)\n# UMAP to reduce dimensionality to two components\numap_ = UMAP(n_components=2,\n             random_state=5,\n             metric=\"cosine\",\n             n_neighbors=50,\n             min_dist=0.1)\n\nvecs_sc = scaler.fit_transform(vecs)\nvecs_pca = pca_.fit_transform(vecs_sc)\nvecs_umap = umap_.fit_transform(vecs_pca)\n\n\n# Assign components to dataset\ncountry.loc[:,\"COMP_1\"] = vecs_umap[:,0]\ncountry.loc[:,\"COMP_2\"] = vecs_umap[:,1]\n\n# Show data\ncountry.head()\n\n/tmp/ipykernel_56168/4075971638.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/tmp/ipykernel_56168/4075971638.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nuser_screen_name\ncountry\ntext\ntext_clean\ntext_cluster\nCOMP_1\nCOMP_2\n\n\n\n\n0\netorrescobo\nEcuador\nJuan Carlos, pero si atrás del video están ust...\nJuan Carlos, pero si atrás del video están ust...\nJuan Carlos, pero si atrás del video están ust...\n6.925628\n9.224609\n\n\n1\netorrescobo\nEcuador\n@ViTTO095 @AndresSeminario Se les escapó entre...\n$MENTION$ $MENTION$ Se les escapó entre chisme...\n@ViTTO095 @AndresSeminario Se les escapó entre...\n7.030454\n7.164153\n\n\n2\netorrescobo\nEcuador\nSolidaridad con @LeninArtieda y con @ecuavisa....\nSolidaridad con $MENTION$ y con $MENTION$. Imp...\nSolidaridad con @LeninArtieda y con @ecuavisa....\n8.251235\n4.659515\n\n\n3\netorrescobo\nEcuador\nFuerza Ecuador. « MoniVelasquezV: 🔴 Te salvé y...\nFuerza Ecuador. « MoniVelasquezV: :red_circle:...\nFuerza Ecuador. « MoniVelasquezV: 🔴 Te salvé y...\n5.924327\n8.702713\n\n\n4\netorrescobo\nEcuador\nLuchito fue para la Asamblea Nacional casi com...\nLuchito fue para la Asamblea Nacional casi com...\nLuchito fue para la Asamblea Nacional casi com...\n7.050543\n8.205220\n\n\n\n\n\n\n\n\n# Visualize the distribution of embeddings with a hover to see the tweets.\nfig = px.scatter(\n    country,\n    x=\"COMP_1\",\n    y=\"COMP_2\",\n    hover_data=[\"user_screen_name\", \"text_clean\"])\nfig.write_html(f\"{assets}/ecuador/ecuador.html\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nClustering\n\n# Apply the HDBSCAN algorithm to extract the clusters.\n\nmc_size = 89\nm_samples = 1\n\ndb_model = HDBSCAN(min_cluster_size=mc_size,\n                   min_samples=m_samples,\n                   metric=\"euclidean\", \n                   )\ndb_model.fit(country.loc[:,[\"COMP_1\", \"COMP_2\"]])\ncountry.loc[:, \"CLUSTER\"] = db_model.labels_\ncountry.loc[:, \"PROBA\"] = db_model.probabilities_\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n/tmp/ipykernel_56168/544355622.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/tmp/ipykernel_56168/544355622.py:12: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Visualize clusters\nplt.figure(figsize=(12,9))\nplt.scatter(vecs_umap[:,0], vecs_umap[:,1], alpha=0.7, c=db_model.labels_, s=10)\nplt.suptitle(f\"Publicaciones de antiderechos en Ecuador\\nMin cluster size: {mc_size} | Min samples: {m_samples}\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/ecuador/ecuador_clusters\")\nplt.show()\n\n\n\n\n\n# Extract the centroid of each cluster\n\ncl_centroids = {}\n\nfor i in sorted(country[\"CLUSTER\"].unique()):\n\n  cl_centroids[i] = country[country[\"CLUSTER\"] == i][[\"COMP_1\", \"COMP_2\"]].mean().values\n\ncentroids_array = np.array(list(cl_centroids.values()))\n\nfig, ax = plt.subplots(figsize=(12,9))\nax.scatter(vecs_umap[:,0], vecs_umap[:,1], s=10, alpha=0.7, c=db_model.labels_)\nax.scatter(centroids_array[:,0], centroids_array[:,1], marker=\"*\", c=\"red\")\nfor i in cl_centroids:\n  ax.annotate(i, cl_centroids.get(i)+np.array([0.1, 0.1]))\nplt.suptitle(f\"Centroides de {len(cl_centroids)} clusters de antiderechos en Ecuador\", fontsize=16)\nplt.tight_layout()\nplt.savefig(f\"{assets}/ecuador/ecuador_centroids\", dpi=fig.dpi)\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nN = 20\n\nwith open(f\"{assets}/ecuador/ecuador.txt\", 'w') as f:\n  for i in cl_centroids:\n    point = cl_centroids.get(i).reshape(1,-1)\n    ix_min_dist = np.argsort(euclidean_distances(point, country.loc[:, [\"COMP_1\", \"COMP_2\"]])).flatten()[:N]\n    print(f\"\\n[Tweets más representativos del cluster {i}]\", file=f)\n    users = country.iloc[ix_min_dist, country.columns.get_loc(\"user_screen_name\")].values\n    texts = country.iloc[ix_min_dist, country.columns.get_loc(\"text_cluster\")].values\n    tweets = [[x, y] for x, y in zip(users, texts)]\n    print('\\n'.join([': '.join(map(str, inner_list)) for inner_list in tweets]), file=f)"
  },
  {
    "objectID": "pages/misionpaz.html",
    "href": "pages/misionpaz.html",
    "title": "Análisis de tweets de @misionpaz_",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2010-02-06 hasta 2023-03-21\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 17450 entries, 179250 to 196699\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                         \n---  ------                   --------------  -----                         \n 0   query                    17450 non-null  object                        \n 1   id                       17450 non-null  float64                       \n 2   timestamp_utc            17450 non-null  int64                         \n 3   local_time               17450 non-null  object                        \n 4   user_screen_name         17450 non-null  object                        \n 5   text                     17450 non-null  object                        \n 6   possibly_sensitive       15102 non-null  object                        \n 7   retweet_count            17450 non-null  float64                       \n 8   like_count               17450 non-null  float64                       \n 9   reply_count              17450 non-null  float64                       \n 10  impression_count         620 non-null    object                        \n 11  lang                     17450 non-null  object                        \n 12  to_username              100 non-null    object                        \n 13  to_userid                100 non-null    float64                       \n 14  to_tweetid               87 non-null     float64                       \n 15  source_name              17450 non-null  object                        \n 16  source_url               17450 non-null  object                        \n 17  user_location            17450 non-null  object                        \n 18  lat                      9 non-null      object                        \n 19  lng                      9 non-null      object                        \n 20  user_id                  17450 non-null  object                        \n 21  user_name                17450 non-null  object                        \n 22  user_verified            17450 non-null  float64                       \n 23  user_description         17450 non-null  object                        \n 24  user_url                 17450 non-null  object                        \n 25  user_image               17450 non-null  object                        \n 26  user_tweets              17450 non-null  object                        \n 27  user_followers           17450 non-null  float64                       \n 28  user_friends             17450 non-null  object                        \n 29  user_likes               17450 non-null  float64                       \n 30  user_lists               17450 non-null  float64                       \n 31  user_created_at          17450 non-null  object                        \n 32  user_timestamp_utc       17450 non-null  float64                       \n 33  collected_via            17450 non-null  object                        \n 34  match_query              17450 non-null  float64                       \n 35  retweeted_id             0 non-null      float64                       \n 36  retweeted_user           0 non-null      float64                       \n 37  retweeted_user_id        0 non-null      float64                       \n 38  retweeted_timestamp_utc  0 non-null      object                        \n 39  quoted_id                51 non-null     object                        \n 40  quoted_user              51 non-null     object                        \n 41  quoted_user_id           51 non-null     float64                       \n 42  quoted_timestamp_utc     51 non-null     float64                       \n 43  collection_time          17450 non-null  object                        \n 44  url                      17450 non-null  object                        \n 45  place_country_code       795 non-null    object                        \n 46  place_name               795 non-null    object                        \n 47  place_type               795 non-null    object                        \n 48  place_coordinates        795 non-null    object                        \n 49  links                    10194 non-null  object                        \n 50  domains                  10194 non-null  object                        \n 51  media_urls               7526 non-null   object                        \n 52  media_files              7526 non-null   object                        \n 53  media_types              7526 non-null   object                        \n 54  media_alt_texts          1940 non-null   object                        \n 55  mentioned_names          3066 non-null   object                        \n 56  mentioned_ids            2583 non-null   object                        \n 57  hashtags                 10072 non-null  object                        \n 58  intervention_type        0 non-null      float64                       \n 59  intervention_text        0 non-null      float64                       \n 60  intervention_url         0 non-null      float64                       \n 61  country                  17450 non-null  object                        \n 62  date                     17450 non-null  datetime64[ns, America/Bogota]\ndtypes: datetime64[ns, America/Bogota](1), float64(20), int64(1), object(41)\nmemory usage: 8.5+ MB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\nfb.me                                  3803\ninstagram.com                          1778\nyoutu.be                               1280\now.ly                                  1173\nmisionpaz.org                           624\ntwitter.com                             216\nyoutube.com                             176\nfb.me|ow.ly                             150\nmisionpaz.org|youtu.be                  135\nbit.ly                                   98\npst.cr                                   90\njhonmilton.org                           64\npscp.tv                                  58\nfb.me|youtube.com                        58\nexplosion.misionpaz.org                  56\ninscripciones.genesis.misionpaz.org      38\nwp.me                                    37\ncongresos.misionpaz.org                  23\nmisiónpaz.org                            19\nfb.me|new.livestream.com                 18\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\nmisionpazmicasa           1561\nmisionpaz                 1098\nmpntucasa                 1012\niglesiampn                 887\ndevocional                 881\nmisionpazencasa            604\nfeparagrandesvictorias     514\n20añostransformando        333\nesnuestracasa              327\nenvivo                     320\nmpnenvivo                  278\nfamiliampn                 253\nsomosfamilia               245\nyosoympn                   240\nmpnnuestracasa             236\nexplosioncontundente       220\navivamiento                185\nfiestademilagros           179\nvive                       176\nmpn                        157\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\njohnmiltonr_          712\ngerarydiana           342\njhonmiltonr           270\njoelmanderfield       257\nyoutube               186\nprjhonmilton          171\nce_palace             164\ngissymander           151\nprofetanormasr        117\nmisionpaziglesia       76\nmisionpaz_             75\nsoynormaruiz           45\nmarcobarrientos        45\nnormanormaruiz         41\nfundacionmisionpaz     30\nprgerardoydiana        28\npastorcashluna         25\ncesarfajardosm         24\notonielfont            23\nevancraft              21\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Spanish\nnlp = spacy.load(\"es_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\ndios               6122\npaz                2051\nmisión             1780\nvida               1589\nmisionpazmicasa    1556\nmensaje            1280\ncompleto           1193\ntiempo             1123\nmisionpaz          1100\nconéctate          1086\ndevocional         1033\nmpntucasa          1016\namor               1000\ncelebración         983\nfamilia             926\niglesiampn          891\njesús               851\npm                  809\nesperamos           805\nseñor               793\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n18    1245\n20    1223\n19    1119\n04    1063\n12    1058\n11    1041\n09    1039\n17    1027\n08    1003\n10     996\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nFacebook                    4460\nTwitter Web App             2791\nHootsuite                   2423\nInstagram                   1614\nTwitter Web Client          1341\nPostcron App                 971\nTwitter for iPad             868\nTwitter for Android          682\nTwitter for iPhone           627\nTweetDeck                    290\nSocialGest                   285\nGoogle                       254\nTwitter Media Studio         230\nRepost.social                167\nHootsuite Inc.               165\na Ning Network               106\nRestream.io                   72\nPeriscope                     57\nerased9_3Ud7cuBk0y            39\nerased132190                   3\nUstream.TV                     2\nLinkedIn                       1\nTwitter for Advertisers.       1\nerased138961                   1\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# selection of topics\ntopics = [1]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @misionpaz_, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 17450 tweets de @misionpaz_, alrededor de 2129 hablan sobre temas de género, es decir, cerca del 12.2%\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/mariafdacabal.html",
    "href": "pages/mariafdacabal.html",
    "title": "Análisis de tweets de @MariaFdaCabal",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2012-01-18 hasta 2023-03-21\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32462 entries, 138957 to 171419\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                         \n---  ------                   --------------  -----                         \n 0   query                    32462 non-null  object                        \n 1   id                       32462 non-null  float64                       \n 2   timestamp_utc            32462 non-null  int64                         \n 3   local_time               32462 non-null  object                        \n 4   user_screen_name         32462 non-null  object                        \n 5   text                     32462 non-null  object                        \n 6   possibly_sensitive       15705 non-null  object                        \n 7   retweet_count            32461 non-null  float64                       \n 8   like_count               32461 non-null  float64                       \n 9   reply_count              32461 non-null  float64                       \n 10  impression_count         1206 non-null   object                        \n 11  lang                     32461 non-null  object                        \n 12  to_username              7757 non-null   object                        \n 13  to_userid                7757 non-null   float64                       \n 14  to_tweetid               7498 non-null   float64                       \n 15  source_name              32461 non-null  object                        \n 16  source_url               32461 non-null  object                        \n 17  user_location            32461 non-null  object                        \n 18  lat                      0 non-null      object                        \n 19  lng                      0 non-null      object                        \n 20  user_id                  32461 non-null  object                        \n 21  user_name                32461 non-null  object                        \n 22  user_verified            32461 non-null  float64                       \n 23  user_description         32461 non-null  object                        \n 24  user_url                 32461 non-null  object                        \n 25  user_image               32461 non-null  object                        \n 26  user_tweets              32461 non-null  object                        \n 27  user_followers           32461 non-null  float64                       \n 28  user_friends             32461 non-null  object                        \n 29  user_likes               32461 non-null  float64                       \n 30  user_lists               32461 non-null  float64                       \n 31  user_created_at          32461 non-null  object                        \n 32  user_timestamp_utc       32461 non-null  float64                       \n 33  collected_via            32461 non-null  object                        \n 34  match_query              32461 non-null  float64                       \n 35  retweeted_id             0 non-null      float64                       \n 36  retweeted_user           0 non-null      float64                       \n 37  retweeted_user_id        0 non-null      float64                       \n 38  retweeted_timestamp_utc  0 non-null      object                        \n 39  quoted_id                4421 non-null   object                        \n 40  quoted_user              4421 non-null   object                        \n 41  quoted_user_id           4421 non-null   float64                       \n 42  quoted_timestamp_utc     4421 non-null   float64                       \n 43  collection_time          32461 non-null  object                        \n 44  url                      32461 non-null  object                        \n 45  place_country_code       3 non-null      object                        \n 46  place_name               3 non-null      object                        \n 47  place_type               3 non-null      object                        \n 48  place_coordinates        3 non-null      object                        \n 49  links                    11585 non-null  object                        \n 50  domains                  11585 non-null  object                        \n 51  media_urls               7326 non-null   object                        \n 52  media_files              7326 non-null   object                        \n 53  media_types              7326 non-null   object                        \n 54  media_alt_texts          898 non-null    object                        \n 55  mentioned_names          14806 non-null  object                        \n 56  mentioned_ids            14239 non-null  object                        \n 57  hashtags                 6863 non-null   object                        \n 58  intervention_type        0 non-null      float64                       \n 59  intervention_text        0 non-null      float64                       \n 60  intervention_url         0 non-null      float64                       \n 61  country                  32461 non-null  object                        \n 62  date                     32462 non-null  datetime64[ns, America/Bogota]\ndtypes: datetime64[ns, America/Bogota](1), float64(20), int64(1), object(41)\nmemory usage: 15.9+ MB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\nbit.ly                    1510\nsemana.com                1111\neltiempo.com               660\nmariafernandacabal.com     544\nfacebook.com               467\nbluradio.com               258\ntwitter.com                249\nlafm.com.co                240\now.ly                      228\nelcolombiano.com           220\nyoutu.be                   207\nyoutube.com                205\ncentrodemocratico.com      192\nln.is                      179\nrcnradio.com               177\nwradio.com.co              176\ninstagram.com              176\ncaracol.com.co             175\nelespectador.com           175\ncostanoticias.com          153\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\ncolumna                  481\nsoycabal                 433\nlascosascomoson          196\n100porcientocabal        129\nenvivo                   123\nsoyopositor              122\natención                 120\nvotacd100cabal           118\nalaire                   107\nfarc                      93\nrecomendado               86\nrestituciónsindespojo     77\nurgente                   73\nbogotá                    67\ncolombia                  65\nvocesysonidos             61\nopinión                   60\ncomunidad                 57\nmañanasblu                57\nvenezuela                 55\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nalvarouribevel     507\njorenvilla1        393\njuanmansantos      326\neltiempo           314\npetrogustavo       310\ndrvargasquemba     301\nigonima            295\ncedemocratico      268\nbluradioco         265\nrcnlaradio         236\nrevistasemana      234\npoliciacolombia    225\nelespectador       218\njflafaurie         208\nricardopuentesm    201\ncol_ejercito       189\nalirestrepo        169\nnoticiasrcn        161\nfiscaliacol        158\nyobusgo            157\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Spanish\nnlp = spacy.load(\"es_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\nq              3678\nfarc           2630\ncolombia       2396\npaz            2222\nd              2188\ngobierno       1317\npaís           1240\nsantos         1104\ngracias         850\npetro           841\njusticia        816\nvenezuela       784\nuribe           770\nbogotá          759\nlibertad        716\nvíctimas        708\naños            708\ncolumna         704\npresidente      687\ncolombianos     633\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n10    2424\n12    2238\n11    2224\n09    2190\n08    2136\n20    1822\n18    1815\n13    1813\n21    1788\n14    1756\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for iPhone             14186\nTwitter for BlackBerry®         8291\nTwitter for Android             5049\nTwitter Web Client              2627\nTwitter for BlackBerry           841\nTweetDeck                        396\nTwitter for iPad                 239\nTwitter for  Android             207\nInstagram                        167\nTwitter Web App                   94\nPeriscope                         84\nJetpack.com                       75\nTwitter for Android Tablets       73\nTwitter Media Studio              73\nTwitter for Websites              19\nTwitter for Windows Phone         18\niOS                               10\nTwitlonger                         4\nerased5423693                      4\nMobile Web (M2)                    3\nTwiffo                             1\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# # selection of topics\n# topics = [0, 2, 3]\n\n# keywords_list = []\n# for topic_ in topics:\n#     topic = topic_model.get_topic(topic_)\n#     keywords = [x[0] for x in topic]\n#     keywords_list.append(keywords)\n\n# # flatten list of lists\n# word_list = [item for sublist in keywords_list for item in sublist]\n\n# # use apply method with lambda function to filter rows\n# filtered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\n# percentage = round(100 * len(filtered_df) / len(df), 2\n# print(f\"Del total de {len(df)} tweets de @MariaFdaCabal, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\n\n\nCode\n# # drop rows with 0 values in two columns\n# filtered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# # add a new column with the sum of two columns\n# filtered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# # extract year from datetime column\n# filtered_df['year'] = filtered_df['date'].dt.year\n\n# # remove urls, mentions, hashtags and numbers\n# p.set_options(p.OPT.URL)\n# filtered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# # Create scatter plot\n# fig = px.scatter(filtered_df, x='like_count', \n#                  y='retweet_count',\n#                  size='impressions', \n#                  color='year',\n#                  hover_name='tweet_text')\n\n# # Update title and axis labels\n# fig.update_layout(\n#     title='Tweets talking about gender with most Likes and Retweets',\n#     xaxis_title='Number of Likes',\n#     yaxis_title='Number of Retweets'\n# )\n\n# fig.show()\n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/brasilsemaborto.html",
    "href": "pages/brasilsemaborto.html",
    "title": "Análisis de tweets de @brasilsemaborto",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2009-07-31 hasta 2023-01-01\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1411 entries, 199416 to 200826\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                            \n---  ------                   --------------  -----                            \n 0   query                    1411 non-null   object                           \n 1   id                       1411 non-null   float64                          \n 2   timestamp_utc            1411 non-null   int64                            \n 3   local_time               1411 non-null   object                           \n 4   user_screen_name         1411 non-null   object                           \n 5   text                     1411 non-null   object                           \n 6   possibly_sensitive       623 non-null    object                           \n 7   retweet_count            1411 non-null   float64                          \n 8   like_count               1411 non-null   float64                          \n 9   reply_count              1411 non-null   float64                          \n 10  impression_count         10 non-null     object                           \n 11  lang                     1411 non-null   object                           \n 12  to_username              281 non-null    object                           \n 13  to_userid                281 non-null    float64                          \n 14  to_tweetid               254 non-null    float64                          \n 15  source_name              1411 non-null   object                           \n 16  source_url               1411 non-null   object                           \n 17  user_location            1411 non-null   object                           \n 18  lat                      9 non-null      object                           \n 19  lng                      9 non-null      object                           \n 20  user_id                  1411 non-null   object                           \n 21  user_name                1411 non-null   object                           \n 22  user_verified            1411 non-null   float64                          \n 23  user_description         1411 non-null   object                           \n 24  user_url                 1411 non-null   object                           \n 25  user_image               1411 non-null   object                           \n 26  user_tweets              1411 non-null   object                           \n 27  user_followers           1411 non-null   float64                          \n 28  user_friends             1411 non-null   object                           \n 29  user_likes               1411 non-null   float64                          \n 30  user_lists               1411 non-null   float64                          \n 31  user_created_at          1411 non-null   object                           \n 32  user_timestamp_utc       1411 non-null   float64                          \n 33  collected_via            1411 non-null   object                           \n 34  match_query              1411 non-null   float64                          \n 35  retweeted_id             0 non-null      float64                          \n 36  retweeted_user           0 non-null      float64                          \n 37  retweeted_user_id        0 non-null      float64                          \n 38  retweeted_timestamp_utc  0 non-null      object                           \n 39  quoted_id                6 non-null      object                           \n 40  quoted_user              6 non-null      object                           \n 41  quoted_user_id           6 non-null      float64                          \n 42  quoted_timestamp_utc     6 non-null      float64                          \n 43  collection_time          1411 non-null   object                           \n 44  url                      1411 non-null   object                           \n 45  place_country_code       15 non-null     object                           \n 46  place_name               15 non-null     object                           \n 47  place_type               15 non-null     object                           \n 48  place_coordinates        15 non-null     object                           \n 49  links                    433 non-null    object                           \n 50  domains                  433 non-null    object                           \n 51  media_urls               326 non-null    object                           \n 52  media_files              326 non-null    object                           \n 53  media_types              326 non-null    object                           \n 54  media_alt_texts          21 non-null     object                           \n 55  mentioned_names          207 non-null    object                           \n 56  mentioned_ids            197 non-null    object                           \n 57  hashtags                 638 non-null    object                           \n 58  intervention_type        0 non-null      float64                          \n 59  intervention_text        0 non-null      float64                          \n 60  intervention_url         0 non-null      float64                          \n 61  country                  1411 non-null   object                           \n 62  date                     1411 non-null   datetime64[ns, America/Sao_Paulo]\ndtypes: datetime64[ns, America/Sao_Paulo](1), float64(20), int64(1), object(41)\nmemory usage: 705.5+ KB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\nbit.ly                                                                    94\nbrasilsemaborto.org                                                       87\nyoutube.com                                                               30\nyoutu.be                                                                  29\nwp.me                                                                     28\ninstagram.com                                                             24\nfacebook.com                                                              19\ntwitpic.com                                                               19\nbrasilsemaborto.com.br                                                    11\ngazetadopovo.com.br                                                        8\ncamara.leg.br                                                              8\nwww12.senado.gov.br                                                        8\ntwitpic.com|twitpic.com                                                    4\nfb.me                                                                      3\nitaucinemas.com.br                                                         3\nbrasilsemaborto.wordpress.com                                              3\nveja.abril.com.br                                                          3\nbrasilsemaborto.org|twitter.com|facebook.com|instagram.com|youtube.com     3\nnoticias.cancaonova.com                                                    3\nwww12.senado.leg.br                                                        3\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\nbrasilsemaborto          271\nmarchavirtualpelavida     87\ncodigopenal               73\nstfabortonao              71\npelas2vidas               61\nmulhersimabortonao        35\nmarchapelavida            33\nabortoépreconceito        33\nasduasvidasimportam       33\n10anos                    30\nestatutodonascituro       28\nafavordavida              24\nbrasilpelasduasvidas      16\nverdadepelavida           16\navidadependedoseuvoto     15\nsimàvida                  12\ndiadonascituro            10\nanencefalo                 9\navidaporumfio              9\nstfdiganaoaoaborto         7\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nlenisegarcia       41\nbrasilsemaborto    29\naddthis             8\nrebeccakiesslin     6\nstf_oficial         6\nanabeatrizries      5\nmpf_pgr             4\ncnnoticias          4\nluh_lena            4\nanadep_brasil       4\ngazetadopovo        4\njorgeferraz         4\nveja                3\nalosenado           3\nwagnermoura         3\nangela_gandra       3\njornaldacbn         3\naddtoany            2\nagenciacamara       2\neunicio             2\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Portuguese\nnlp = spacy.load(\"pt_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\nvida                     494\naborto                   306\nbrasilsemaborto          274\nmarcha                   165\nbrasil                   131\nnacional                 110\ndia                      109\nmarchavirtualpelavida     88\nmovimento                 85\nnascituro                 77\nparticipe                 74\ncodigopenal               73\nmãe                       71\ndefesa                    71\nstfabortonao              70\nhoje                      66\nestatuto                  65\ncódigo                    64\nsaiba                     63\nacompanhe                 60\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n11    193\n10    161\n16    130\n15    128\n19    111\n12    106\n09     94\n17     91\n14     72\n18     57\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for Android            479\nTwitter Web Client             453\nPlume for Android              161\nTwitter Web App                 89\nTwitter for iPhone              60\nTweetDeck                       42\nJetpack.com                     41\nTwitter for Websites            26\nTweetCaster for Android         18\nPosterous                       16\nGravity                         15\nTwitter for Android Tablets      7\nGravity!                         2\nTwibbon                          1\nTwitpic                          1\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# selection of topics\ntopics = [0, 2]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @brasilsemaborto, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 1411 tweets de @brasilsemaborto, alrededor de 1020 hablan sobre temas de género, es decir, cerca del 72.29%\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/etorrescobo.html",
    "href": "pages/etorrescobo.html",
    "title": "Análisis de tweets de @etorrescobo",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2010-07-06 hasta 2023-03-21\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 8314 entries, 10617 to 18930\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                            \n---  ------                   --------------  -----                            \n 0   query                    8314 non-null   object                           \n 1   id                       8314 non-null   float64                          \n 2   timestamp_utc            8314 non-null   int64                            \n 3   local_time               8314 non-null   object                           \n 4   user_screen_name         8314 non-null   object                           \n 5   text                     8314 non-null   object                           \n 6   possibly_sensitive       2818 non-null   object                           \n 7   retweet_count            8314 non-null   float64                          \n 8   like_count               8314 non-null   float64                          \n 9   reply_count              8314 non-null   float64                          \n 10  impression_count         243 non-null    object                           \n 11  lang                     8314 non-null   object                           \n 12  to_username              2188 non-null   object                           \n 13  to_userid                2188 non-null   float64                          \n 14  to_tweetid               2120 non-null   float64                          \n 15  source_name              8314 non-null   object                           \n 16  source_url               8314 non-null   object                           \n 17  user_location            8314 non-null   object                           \n 18  lat                      65 non-null     object                           \n 19  lng                      65 non-null     object                           \n 20  user_id                  8314 non-null   object                           \n 21  user_name                8314 non-null   object                           \n 22  user_verified            8314 non-null   float64                          \n 23  user_description         8314 non-null   object                           \n 24  user_url                 8314 non-null   object                           \n 25  user_image               8314 non-null   object                           \n 26  user_tweets              8314 non-null   object                           \n 27  user_followers           8314 non-null   float64                          \n 28  user_friends             8314 non-null   object                           \n 29  user_likes               8314 non-null   float64                          \n 30  user_lists               8314 non-null   float64                          \n 31  user_created_at          8314 non-null   object                           \n 32  user_timestamp_utc       8314 non-null   float64                          \n 33  collected_via            8314 non-null   object                           \n 34  match_query              8314 non-null   float64                          \n 35  retweeted_id             0 non-null      float64                          \n 36  retweeted_user           0 non-null      float64                          \n 37  retweeted_user_id        0 non-null      float64                          \n 38  retweeted_timestamp_utc  0 non-null      object                           \n 39  quoted_id                800 non-null    object                           \n 40  quoted_user              800 non-null    object                           \n 41  quoted_user_id           800 non-null    float64                          \n 42  quoted_timestamp_utc     800 non-null    float64                          \n 43  collection_time          8314 non-null   object                           \n 44  url                      8314 non-null   object                           \n 45  place_country_code       672 non-null    object                           \n 46  place_name               672 non-null    object                           \n 47  place_type               672 non-null    object                           \n 48  place_coordinates        672 non-null    object                           \n 49  links                    1660 non-null   object                           \n 50  domains                  1660 non-null   object                           \n 51  media_urls               1812 non-null   object                           \n 52  media_files              1812 non-null   object                           \n 53  media_types              1812 non-null   object                           \n 54  media_alt_texts          249 non-null    object                           \n 55  mentioned_names          4040 non-null   object                           \n 56  mentioned_ids            3764 non-null   object                           \n 57  hashtags                 1824 non-null   object                           \n 58  intervention_type        0 non-null      float64                          \n 59  intervention_text        0 non-null      float64                          \n 60  intervention_url         0 non-null      float64                          \n 61  country                  8314 non-null   object                           \n 62  date                     8314 non-null   datetime64[ns, America/Guayaquil]\ndtypes: datetime64[ns, America/Guayaquil](1), float64(20), int64(1), object(41)\nmemory usage: 4.1+ MB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\netorrescobo.com             241\ntinyurl.com                 141\ninstagram.com               120\nbit.ly                       85\nyoutu.be                     64\nelcomercio.com               64\nyoutube.com                  52\nabc.es                       52\nft.com                       46\neluniverso.com               40\nelpais.com                   36\now.ly                        30\nfacebook.com                 25\nmedium.com                   24\nexpreso.ec                   23\nwsj.com                      23\ntwitter.com                  20\ninternacional.elpais.com     16\nhoy.com.ec                   13\nnyti.ms                      12\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\necuador                   213\nambato                    163\ntungurahua                116\nquito                      47\nvenezuela                  43\nasambleanacional           35\nmaduro                     29\ntrump                      26\nespaña                     24\nvota6                      23\natención                   21\ncoip                       20\nemprendersinobstáculos     19\nusfq                       16\ncolombia                   16\nbrexit                     15\nestebantorres              14\ncambio                     14\necuadorprotesta            14\ntoros                      13\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nasambleaecuador    235\nestebanperezm      140\netorrescobo        133\nlftorrest           86\nlassoguillermo      74\nbancadapsc          66\neluniversocom       62\nrxandrade           57\nel_pais             52\nusfq_ecuador        48\njfcarpio            47\ncambioec            43\nabc_es              42\namandahidalgoa      39\nxvillalba1          39\necuavisa            37\ncristiano           35\nla6ecuador          35\nyoutube             33\nlenin               31\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Spanish\nnlp = spacy.load(\"es_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\necuador         586\ngracias         388\ngobierno        343\nvía             332\nasamblea        298\nsaludos         280\npaís            274\nambato          265\npresidente      259\nley             251\nartículo        221\nnacional        216\nthe             203\necuatorianos    178\ntungurahua      173\ncomparto        167\naños            161\ndebate          152\nvida            152\nquito           150\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n11    633\n10    587\n12    578\n17    571\n20    552\n09    503\n13    502\n21    490\n16    469\n18    451\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter for iPhone         4322\nTwitter Web Client         1787\nTwitter for iPad            704\nTwitter for Android         432\nTwitter for BlackBerry®     344\nTwitter Web App             232\nTwitter for Websites        229\nInstagram                   105\nKioskoymas                   41\nAgorapulse app               33\nMobile Web                   17\nMedium                       17\niOS                          14\nHootsuite Inc.               13\nTweetChat                     6\nKindle                        5\nCanva                         5\nFOX News Login                2\nPhotos on iOS                 2\nOS X                          1\nInstagram on iOS              1\nCrowdfire Inc.                1\nbitly bitlink                 1\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF ### Tópicos\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# selection of topics\ntopics = [3]\n\nkeywords_list = []\nfor topic_ in topics:\n    topic = topic_model.get_topic(topic_)\n    keywords = [x[0] for x in topic]\n    keywords_list.append(keywords)\n\n# flatten list of lists\nword_list = [item for sublist in keywords_list for item in sublist]\n\n# use apply method with lambda function to filter rows\nfiltered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\npercentage = round(100 * len(filtered_df) / len(df), 2)\nprint(f\"Del total de {len(df)} tweets de @etorrescobo, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\nDel total de 8314 tweets de @etorrescobo, alrededor de 4343 hablan sobre temas de género, es decir, cerca del 52.24%\n\n\n\n\nCode\n# drop rows with 0 values in two columns\nfiltered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# add a new column with the sum of two columns\nfiltered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# extract year from datetime column\nfiltered_df['year'] = filtered_df['date'].dt.year\n\n# remove urls, mentions, hashtags and numbers\np.set_options(p.OPT.URL)\nfiltered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# Create scatter plot\nfig = px.scatter(filtered_df, x='like_count', \n                 y='retweet_count',\n                 size='impressions', \n                 color='year',\n                 hover_name='tweet_text')\n\n# Update title and axis labels\nfig.update_layout(\n    title='Tweets talking about gender with most Likes and Retweets',\n    xaxis_title='Number of Likes',\n    yaxis_title='Number of Retweets'\n)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  },
  {
    "objectID": "pages/pastormalafaia.html",
    "href": "pages/pastormalafaia.html",
    "title": "Análisis de tweets de @PastorMalafaia",
    "section": "",
    "text": "Información\nLos datos de este usuario cubren desde la creación de la cuenta 2010-06-11 hasta 2023-03-20\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 43709 entries, 45185 to 88893\nData columns (total 63 columns):\n #   Column                   Non-Null Count  Dtype                            \n---  ------                   --------------  -----                            \n 0   query                    43709 non-null  object                           \n 1   id                       43709 non-null  float64                          \n 2   timestamp_utc            43709 non-null  int64                            \n 3   local_time               43709 non-null  object                           \n 4   user_screen_name         43709 non-null  object                           \n 5   text                     43709 non-null  object                           \n 6   possibly_sensitive       26733 non-null  object                           \n 7   retweet_count            43709 non-null  float64                          \n 8   like_count               43709 non-null  float64                          \n 9   reply_count              43709 non-null  float64                          \n 10  impression_count         365 non-null    object                           \n 11  lang                     43709 non-null  object                           \n 12  to_username              874 non-null    object                           \n 13  to_userid                874 non-null    float64                          \n 14  to_tweetid               560 non-null    float64                          \n 15  source_name              43709 non-null  object                           \n 16  source_url               43709 non-null  object                           \n 17  user_location            43709 non-null  object                           \n 18  lat                      0 non-null      object                           \n 19  lng                      0 non-null      object                           \n 20  user_id                  43709 non-null  object                           \n 21  user_name                43709 non-null  object                           \n 22  user_verified            43709 non-null  float64                          \n 23  user_description         43709 non-null  object                           \n 24  user_url                 43709 non-null  object                           \n 25  user_image               43709 non-null  object                           \n 26  user_tweets              43709 non-null  object                           \n 27  user_followers           43709 non-null  float64                          \n 28  user_friends             43709 non-null  object                           \n 29  user_likes               43709 non-null  float64                          \n 30  user_lists               43709 non-null  float64                          \n 31  user_created_at          43709 non-null  object                           \n 32  user_timestamp_utc       43709 non-null  float64                          \n 33  collected_via            43709 non-null  object                           \n 34  match_query              43709 non-null  float64                          \n 35  retweeted_id             0 non-null      float64                          \n 36  retweeted_user           0 non-null      float64                          \n 37  retweeted_user_id        0 non-null      float64                          \n 38  retweeted_timestamp_utc  0 non-null      object                           \n 39  quoted_id                53 non-null     object                           \n 40  quoted_user              53 non-null     object                           \n 41  quoted_user_id           53 non-null     float64                          \n 42  quoted_timestamp_utc     53 non-null     float64                          \n 43  collection_time          43709 non-null  object                           \n 44  url                      43709 non-null  object                           \n 45  place_country_code       21 non-null     object                           \n 46  place_name               21 non-null     object                           \n 47  place_type               21 non-null     object                           \n 48  place_coordinates        21 non-null     object                           \n 49  links                    20062 non-null  object                           \n 50  domains                  20062 non-null  object                           \n 51  media_urls               10154 non-null  object                           \n 52  media_files              10154 non-null  object                           \n 53  media_types              10154 non-null  object                           \n 54  media_alt_texts          399 non-null    object                           \n 55  mentioned_names          3473 non-null   object                           \n 56  mentioned_ids            3149 non-null   object                           \n 57  hashtags                 2265 non-null   object                           \n 58  intervention_type        0 non-null      float64                          \n 59  intervention_text        0 non-null      float64                          \n 60  intervention_url         0 non-null      float64                          \n 61  country                  43709 non-null  object                           \n 62  date                     43709 non-null  datetime64[ns, America/Sao_Paulo]\ndtypes: datetime64[ns, America/Sao_Paulo](1), float64(20), int64(1), object(41)\nmemory usage: 21.3+ MB\n\n\n\n\nDatos\n\n\nCode\n# count items on column\ndomains_list = df['domains'].value_counts()\n\n# return first n rows in descending order\ntop_domains = domains_list.nlargest(20)\n\ntop_domains\n\n\ndomains\nyoutu.be                       7145\nverdadegospel.com              3659\nyoutube.com                    2116\ngoo.gl                         1719\nveja.abril.com.br               992\nvitoriaemcristo.org             848\nmigre.me                        696\neditoracentralgospel.com        336\nbit.ly                          166\ninstagram.com                   153\ngospelplay.com                  147\nfacebook.com                    140\nfacebook.com|youtube.com        123\npastoresjuntos.com              111\neventos.vitoriaemcristo.org     103\nfb.watch|instagram.com          103\nescoladelideresonline.com        69\ng1.globo.com                     67\nadvec.org                        67\neventospastorsilas.com.br        66\nName: count, dtype: int64\n\n\n\n\nHashtags\nLista del top 20 de hashtags más usados y su frecuencia\n\n\nCode\n# convert dataframe column to list\nhashtags = df['hashtags'].to_list()\n\n# remove nan items from list\nhashtags = [x for x in hashtags if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nhashtags = [x.split('|') for x in hashtags]\n\n# flatten list of lists\nhashtags = [item for sublist in hashtags for item in sublist]\n\n# count items on list\nhashtags_count = pd.Series(hashtags).value_counts()\n\n# return first n rows in descending order\ntop_hashtags = hashtags_count.nlargest(20)\n\ntop_hashtags\n\n\nsilasmalafaia                       203\nroubalheiraepttudoaver              197\ndilmavaiperderaecio45vencer         123\nelessabiamdorouboforadilma          116\n12anosderoubalheiradoptchega         71\npovobrasileirocontradilmaept         70\nchegaderoubalheiraforadilma          69\n225                                  60\nchegaderouboementiraforadilma        59\ndilmanaodialoguecomterrorista        54\nclamandopelobrasil                   52\ndilmaroubalheiraepttudoaver          52\nfachinnão                            46\nnemcorrupçãonemptnemdilma            43\nvideodepregacao                      43\nrespond                              43\naovivocommalafaia                    42\nlulaedilmamenosodiocontramarina      42\nvotoaeciopelobr45il                  41\nmarinaresistentevaiserpresidente     39\nName: count, dtype: int64\n\n\n\n\nUsuarios\nTop 20 de usuarios más mencionados en los tweets\n\n\nCode\n# filter column from dataframe\nusers = df['mentioned_names'].to_list()\n\n# remove nan items from list\nusers = [x for x in users if not pd.isna(x)]\n\n# split items into a list based on a delimiter\nusers = [x.split('|') for x in users]\n\n# flatten list of lists\nusers = [item for sublist in users for item in sublist]\n\n# count items on list\nusers_count = pd.Series(users).value_counts()\n\n# return first n rows in descending order\ntop_users = users_count.nlargest(20)\n\ntop_users\n\n\nverdadegospel            638\nadvecoficial             532\nedcentralgospel          179\nreinaldoazevedo          158\navec_oficial             135\nelizetemalafaia          115\nradaronline              103\neyshila1                  70\ncgospelmusic              60\npastormalafaiaoficial     52\npastormalafaia            45\nnani_azevedo              43\ndrmikemurdock             41\nsilasmalafaia             39\nveja                      38\njozyanneoficial           37\nmagnomaltaofc             35\nadvecsaopaulo             33\npgm_ratinho               26\ndanilogentili             23\nName: count, dtype: int64\n\n\n\n\nLikes en el tiempo\n\n\nCode\n# plot the data using plotly\nfig = px.line(df, \n              x='date', \n              y='like_count', \n              title='Likes over Time',\n              template='plotly_white', \n              hover_data=['text'])\n\n# show the plot\nfig.show()\n\n\n\n                                                \n\n\n\n\nTokens\nLista del top 20 de los tokens más comunes y su frecuencia\n\n\nCode\n# load the spacy model for Portuguese\nnlp = spacy.load(\"pt_core_news_sm\")\n\n# load stop words for Spanish\nSTOP_WORDS = nlp.Defaults.stop_words\n\n# Function to filter stop words\ndef filter_stopwords(text):\n    # lower text\n    doc = nlp(text.lower())\n    # filter tokens\n    tokens = [token.text for token in doc if not token.is_stop and token.text not in STOP_WORDS and token.is_alpha]\n    return ' '.join(tokens)\n\n# apply function to dataframe column\ndf['text_pre'] = df['text'].apply(filter_stopwords)\n\n# count items on column\ntoken_counts = df[\"text_pre\"].str.split(expand=True).stack().value_counts()[:20]\n\ntoken_counts\n\n\nassista       7856\nq             5381\nvídeo         4499\ndeus          3435\nprograma      3030\ndia           2894\nbolsonaro     2840\nvitória       2782\ncristo        2620\nbrasil        2418\nacesse        2256\nhoje          2189\nvou           2168\npt            2073\ndivulgue      1981\nñ             1882\nsábado        1816\nimprensa      1727\nlula          1721\nimperdível    1715\nName: count, dtype: int64\n\n\n\n\nHora\nLista de las 10 horas con más cantidad de tweets publicados\n\n\nCode\n# extract hour from datetime column\ndf['hour'] = df['date'].dt.strftime('%H')\n\n# count items on column\nhours_count = df['hour'].value_counts()\n\n# return first n rows in descending order\ntop_hours = hours_count.nlargest(10)\n\ntop_hours\n\n\nhour\n17    3772\n12    3693\n14    3432\n15    3366\n16    3268\n10    3017\n11    3004\n13    2849\n18    2446\n19    2377\nName: count, dtype: int64\n\n\n\n\nPataformas\nPlataformas desde las que se publicaron contenidos y su frecuencia\n\n\nCode\ndf['source_name'].value_counts()\n\n\nsource_name\nTwitter Web Client                 11191\nPostcron App                       10752\nTwitter for iPad                    8900\nmLabs - Gestão de Redes Sociais     7243\nTwitter for iPhone                  2183\nerased3412752                        723\nTwitter Ads                          580\nTwitter for Android                  466\nTwitter for Android Tablets          444\nTweetDeck                            424\nTwitter Web App                      303\nPostgrain                            144\nPeriscope                            106\nTwitter for BlackBerry®               98\nTwitter for Advertisers.              65\nDynamic Tweets                        63\nTwitpic                                7\nTwitter for Websites                   3\nMobile Web                             3\niOS                                    3\nMobile Web (M2)                        2\nInstagram                              2\nPhotos on iOS                          1\nTwitter Media Studio                   1\naudioBoom                              1\nTwitter for Windows Phone              1\nName: count, dtype: int64\n\n\n\n\nTópicos\nTécnica de modelado de tópicos con transformers y TF-IDF\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nReducción de tópicos\nMapa con 10 tópicos del contenido de los tweets\n\n\nCode\n# visualize topics\ntopic_model.visualize_topics()\n\n\n\n                                                \n\n\n\n\nTérminos por tópico\n\n\nCode\ntopic_model.visualize_barchart(top_n_topics=11)\n\n\n\n                                                \n\n\n\n\nAnálisis de tópicos\nSelección de tópicos que tocan temas de género\n\n\nCode\n# # selection of topics\n# topics = [0]\n\n# keywords_list = []\n# for topic_ in topics:\n#     topic = topic_model.get_topic(topic_)\n#     keywords = [x[0] for x in topic]\n#     keywords_list.append(keywords)\n\n# # flatten list of lists\n# word_list = [item for sublist in keywords_list for item in sublist]\n\n# # use apply method with lambda function to filter rows\n# filtered_df = df[df['text_pre'].apply(lambda x: any(word in x for word in word_list))]\n\n# percentage = round(100 * len(filtered_df) / len(df), 2)\n# print(f\"Del total de {len(df)} tweets de  @PastorMalafaia, alrededor de {len(filtered_df)} hablan sobre temas de género, es decir, cerca del {percentage}%\")\n\n\n\n\nCode\n# # drop rows with 0 values in two columns\n# filtered_df = filtered_df[(filtered_df.like_count != 0) & (filtered_df.retweet_count != 0)]\n\n# # add a new column with the sum of two columns\n# filtered_df['impressions'] = (filtered_df['like_count'] + filtered_df['retweet_count'])/2\n\n# # extract year from datetime column\n# filtered_df['year'] = filtered_df['date'].dt.year\n\n# # remove urls, mentions, hashtags and numbers\n# p.set_options(p.OPT.URL)\n# filtered_df['tweet_text'] = filtered_df['text'].apply(lambda x: p.clean(x))\n\n# # Create scatter plot\n# fig = px.scatter(filtered_df, x='like_count', \n#                  y='retweet_count',\n#                  size='impressions', \n#                  color='year',\n#                  hover_name='tweet_text')\n\n# # Update title and axis labels\n# fig.update_layout(\n#     title='Tweets talking about gender with most Likes and Retweets',\n#     xaxis_title='Number of Likes',\n#     yaxis_title='Number of Retweets'\n# )\n\n# fig.show()\n\n\n\n\nTópicos en el tiempo\n\n\nCode\n# convert column to list\ntweets = df['text_pre'].to_list()\ntimestamps = df['local_time'].to_list()\n\ntopics_over_time = topic_model.topics_over_time(docs=tweets, \n                                                timestamps=timestamps, \n                                                global_tuning=True, \n                                                evolution_tuning=True, \n                                                nr_bins=20)\n\ntopic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
  }
]